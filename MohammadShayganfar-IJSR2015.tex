%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{varwidth}
\usepackage{setspace}
\usepackage{perpage}
\usepackage{enumerate}
\MakePerPage{footnote}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Toward Improving Human-Robot Collaboration with Emotional Awareness}%\thanks{Grants or
% other notes about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
%}

%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Mohammad Shayganfar \and
        Charles Rich \and
        Candace L. Sidner
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Mohammad Shayganfar \and Charles Rich \and Candace L. Sidner \at
              100 Institute Road, Worcester, MA, USA 01609-2280 \\
              Tel.: +1 508-831-5357\\
              Fax: +1 508-831-5776\\
              \email{mshayganfar@wpi.edu}\\
              \email{rich@wpi.edu}\\
              \email{sidner@wpi.edu}\\
%             \emph{Present address:} of F. Author  %  if needed
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}\ldots


\keywords{Human-Robot Collaboration \and Emotion-Awareness \and Affective
Motivational Collaboration Theory}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

- The importance of understanding collaboration.

\noindent- The importance of understanding underlying processes of the
collaboration process.

\section{Example Scenario}
\label{sec:example-scenario}
%Text with citations \cite{RefB} and \cite{RefJ}.

\subsection{The Backstory}

The scenario transpires in a NASA's research center. Light, temperature and
other environmental factors are simulated based on conditions on the surface of
the moon. The mission is to finish installing the required solar panels to
provide energy for the operation of NASA's science lab on the moon. Ninety
percent of these panels have already been installed. However, the operation is
now faced with low batteries which forces everyone to be cautious about
consuming energy. The astronaut is inspecting the working conditions in the
field and planning the installation of the remaining panels in collaboration
with the robot. He determines that the sun will cast shadows over the
installation structure, leading to potential difficulties. The astronaut asks
control base to go through the final checks of the robot and prepare it for the
operation.

\subsection{Astronaut-Robot Interaction}

The robot and the astronaut will collaborate with each other to achieve their
shared goal, which is to install two solar panels. They will face various
difficulties, ranging from the task being unpleasant and challenging to
conflicts of their private and/or shared goals occurring because of a blocked or
a protracted sub-task. The robot and the astronaut will go through a series of
assessment processes to figure out a) how did the current blocking happen? b)
why is the current task is blocked? and c) what is the next action they are
going to take? The robot uses its cognitive abilities and its communication
skills to overcome these problems and to motivate the astronaut to propose
alternative tasks. The following is part of an interaction between the astronaut
and the robot during their collaboration on installing solar panels.

\subsection{Agreeing on Shared Goal (Emotion-Awareness)}
\label{sec:exp1}

This and the next hypothetical examples show that agreeing on a shared goal
requires the Robot to be aware of its collaborator's emotions (here,
frustration). In this example, the Astronaut's first turn (A1), shows her
verbally conveying her frustration with respect to the disfunctioning
measurement tool that is used for checking the quality of the installed panel.
In return, the Robot's first turn (A2), as the crucial part of this interaction,
shows the Robot perceiving the Astronaut's frustration and acknowledging that
verbally.\footnote{The underlined section of the Robot's utterances (in turn A2)
shows the influence of using emotion-driven processes which leads to
acknowledgement of the Astronaut's emotion. See the absence of these utterances
as the consequence of ignoring the Astronaut's emotions in the same turn in the
next example (Section \ref{sec:exp2}).} Later on, in Section \ref{sec:wt-exp1},
we are going to show how the computational mechanisms, discussed in Section
\ref{sec:AMCT}, are involved in this process. In other words, we are going to
discuss how the emotion-driven goal-directed mechanisms can work together and
lead the Robot's behavior to acknowledge the perceived emotion of the Astronaut
properly, in order to avoid termination of the collaboration. Continuing in turn
A3, the Astronaut's utterance shows the change of the underlying belief from
termination of the collaboration to a belief in the possibility of seeking
instrumental support by asking the Robot whether it is possible to fix the
measurement tool. Notice that the proper acknowledgement of the Astronaut's
emotion helps to change her emotion from frustration to neutral. Now that the
Astronaut does not express a negative emotion (i.e., frustration), and she is
asking for instrumental support, the Robot can provide the alternative task as a
potential solution (A4). Here is another advantage of the emotion-awareness in
this hypothetical example. Although, the Robot, according to the shared plan
(see Sections \ref{sec:collaboration-theory} and \ref{sec:AMCT}), could provide
the same alternative task as a solution to the Astronaut immediately, it
procrastinated instead, providing the potential solution based on the
Astronaut's negative emotional state, i.e., frustration. Finally, since agreeing
on a shared goal is a collaborative negotiation process, emotion-awareness plays
a crucial role in providing a fair offer to the collaborator during negotiation.
As a result, the Astronaut's response in the last turn (A5) shows the acceptance
of the Robot's potential solution to continue collaboration and agreement on the
shared goal while she is content. In the next example we are going to show what
happens to the same hypothetical example when the Robot ignores the Astronaut's
emotion and tries to save the collaboration process from failure.\\

\begin{spacing}{1.3}
\small{ 
\begin{description}
  \item \textit{\textbf{A1. Astronaut:}} Oh no! Finishing the quality check of
  our installation with this measurement problem is so frustrating. I think we
  should stop now!\\

  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{A2. Robot:}}
  \underline{I see. This is frustrating.} But, I can help you with the
  measurement tool and we can finish the task as originally planned.
  \end{varwidth}}\\
  
  \item \textit{\textbf{A3. Astronaut:}} Can you fix the measurement tool?\\

  \item \textit{\textbf{A4. Robot:}} The next task is fixing the panel and it
  needs you to prepare and attach the welding rod to your welding tool. To save
  our time, I will fetch another measurement tool while you are preparing your
  welding tool.\\

  \item \textit{\textbf{A5. Astronaut:}} That would be great!
  
\end{description}
}
\end{spacing}

\subsection{Agreeing on Shared Goal (Emotion-Ignorance)}
\label{sec:exp2}

This example shows the same process of agreeing on a shared goal as previous
one except that it diverges from reaching to an agreement, despite the fact
that it begins with the same utterance (B1) as it appears in previous example
(A1). As mentioned earlier in Section \ref{sec:exp1}, the emotion-awareness is
beneficial in collaboration by channeling the collaboration process towards the
shared goal in the right direction. Without the emotion-awareness a
collaborative Robot will try to maintain the status of the shared goal and
prevent it from failure without considering its collaborator's negative emotion
which can be a direct result of a type of task failure during collaboration.
First, the emotion-ignorant Robot does not acknowledge the Astronaut's
frustration (i.e., B2 in compare to A2 in Section \ref{sec:exp1}), since it does
not perceive that emotion. Then, while negotiating the shared goal the Robot
fails to offer a potential solution with respect to the Astronaut's emotional
state which is the reflection of Astronaut's overall mental state. As a result,
it causes the failure of the negotiation procedure during collaboration.

In this example, the Robot is not capable of perceiving the Astronaut's emotion,
thus it does not apply the Astronaut's emotion (i.e., frustration) as an
influential factor in its computational mechanisms (see Section \ref{sec:AMCT}).
Hence, in the Robot's first response to the Astronaut's utterances (B2), first
it does not acknowledge the Astronaut' emotion, and second, it immidiately
conveys two available alternative actions according to the existing shared plan
(see Section \ref{sec:collaboration-theory}) and asks the Astronaut to select
between these two actions.\footnote{Notice that the Robot's acknowledgement of
the Astronaut's emotion is missing in B2 because of ignoring the Astronaut's
emotion -- see the Robot's same turn (A2) in Section \ref{sec:exp1}. Also,
notice (for the same reason) that the underlined uttrance in B2, reveals that
the Robot requires the Astronaut's confirmation for the next step.} As it
appears in the Astronaut's response, the Robot's immidiate proposal does not
make any progress in collaboration. As a result, the Astronaut just repeats
himself about the task status (B3) while still expresses frustration. The
Astronaut's response does not change the Robot's mental state and this causes
the Robot to try to repeat its own question while still is missing the
Astronaut's frustration (B4). The Robot's utterance creates an ambiguious
assumption for the Astronaut on whether the Robot can fix the broken measurement
tool for her. This ambiguity makes the Astronaut even more frustrated and causes
her to ask a question to remove the ambiguity of the Robot's proposal (B5). In
return, the Robot not only misses the Astronaut's intensified frustration, but
it also nullifies the Astronaut's assumption about fixing the disfunctional
measurement tool and proposes the potentioal solution of replacing the broken
tool and seeing whether the Astronaut agrees on that (B6). In B7, the Astronaut
modifies its assumption and announces the shortage of time as justification of
expressing her anger. At this point, the Robot's response becomes more crucial
since its wrong way of interaction and emotion-ignorance behavior shift the
Astronaut's emotional and mental states into a noncollaborative status.
Consequently, again, the Robot wants to revive the collaboration process,
therefore it provides more information about the repercussions of terminating
the collaboration process and wants to see whether the Astronaut can pursue
another task (B8).\footnote{Notice the underlined section of the Robot's turn
B8 indicates its dissociated reasoning about the problem from the Astronaut's
mental state.} Finally, the incorrect interaction of the Robot caused by its
emotion-ignorance behaviors which was developed since the Robot's very first
response and has continued till the end, leads to an unsuccessful termination of
their collaboration (B9).\\

\begin{spacing}{1.2}
\small{
\begin{description}
  \item \textit{\textbf{B1. Astronaut:}} Oh no! Finishing the quality check of
  our installation with this measurement problem is so frustrating. I think we
  should stop now!\\

  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{B2. Robot:}} I can
  help you with the measurement tool, or we can terminate this task.
  \underline{What do you want me to do?}\end{varwidth}}\\
  
  \item \textit{\textbf{B3. Astronaut:}} As I said the measurement tool does not
  work properly. We can not continue!\\

  \item \textit{\textbf{B4. Robot:}} Okay. Do you want me to fix this problem
  or terminate the task?\\

  \item \textit{\textbf{B5. Astronaut:}} Can you fix my measurement tool?\\
  
  \item \textit{\textbf{B6. Robot:}} I cannot fix your measurement tool, but I
  can fetch another one for you if you want?\\
  
  \item \textit{\textbf{B7. Astronaut:}} No, I don't want another measurement tool!
  We don't have time for that!\\
  
  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{B8. Robot:}} Okay.
  You want me to terminate this task. Terminating this task can influence the
  quality of installing this solar panel which can cause the mission to fail.
  Or, \underline{do you want us to work on another task?} This can help us to
  install the panel using your welding tool, but I do not know whether the
  quality of our installation will be acceptable.\end{varwidth}}\\
  
  \item \textit{\textbf{B9. Astronaut:}} I told you we have this problem and we
  should terminate the mission! We cannot continue without the measurement
  tool!
\end{description}
}
\end{spacing}

\subsection{Delegation of a Task (Emotion-Awareness)}
\label{sec:exp3}

This and the next hypothetical examples as a completely different collaborative
behaviors show how delegation of a task critically depends on understanding how
worried the other collaborator is and the necessity of having sufficient time,
which play together. This example shows when the Robot is aware of the
Astronaut's worriedness, it can use its own motivation mechanism drived by
emotions to come up with a way to alleviate that. Its methods are to exactly
postpone any questions until such time as they are critical (effects are
appeared in C2, C4). At the beginning of this example (C1), Astronaut is worried
because of the lack of time in achieving the shared goal (finishing to install
solar panels) for the overall task. She proposes the Robot to begin installing
the second panel, since the first one still has some problems. The Robot in its
first turn (C2), perceives the Astronaut's emotion (i.e., worriedness) and using
the same cognitive mechanisms (see Section \ref{sec:AMCT}) acknowledges the
Astronaut's emotion just as it did in first example in Section
\ref{sec:exp1}.\footnote{The underlined utterance in the Robot's turn C2, shows
the Robot's awareness of the Astronaut's emotion.} Also, Robot does not ask the
Astronaut (because of perceiving worriedness) if it is okay to leave the current
task which was helping the Astronaut to install the first panel. As mentioned
earlier, we are going to show step-by-step (see Sections \ref{sec:wt-exp1} and
\ref{sec:wt-exp3}) how different emotion-driven goal-directed mechanisms get
invloved in each turn of the collaboration process to appropriately respond to
the Astronaut based on the \textit{Affective Motivational Collaboration Theory}
(see Section \ref{sec:AMCT}). Continuing in this example, after acknowledging
the Astronaut's emotion, the Robot infers that it needs to postpone asking
questions about the missing parts of the shared plan (see Section
\ref{sec:collaboration-theory}) since installing a panel is a collaborative task
and some of the primitive tasks need to be done by the Astronaut. Then, the
Astronaut perceives the Robot's response as a proper acceptance of the task
delegation and tries to communicate the status of her own, even though she is
still worried about finishing the overall task (C3). Now, the Robot perceives
the Astronaut's worriedness and without asking the actual detailed questions
about the delegated task, declares the possibility of asking some follow-up
questions whenever the Astronaut's answers are required to make progress in
executing some primitive tasks while installing the second panel (C4). Here, the
Robot not only prevents overwhelming the Astronaut with several questions
related to executing the next steps in the future, but its utterance implicitly
reveals its knowledge of tasks' requirements to the Astronaut. As a result, the
Robot's appraoch on acceptance of the delegated task mitigates the Astronaut's
negative emotion (i.e., worriedness) which makes her to respond possitively to
the Robot's proposal. The next example explains the same hypothetical task
delegation scenario except it shows the negative impact of missing the
Astronaut's emotion on task delegation process.\\

\begin{spacing}{1.2}
\small{
\begin{description}
  \item \textit{\textbf{C1. Astronaut:}} I still have some problems with
  attaching the first panel! We do not have enough time. You should begin to
  install the second panel.\\

  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{C2. Robot:}} Okay.
  \underline{Don't worry.} I can handle that.

  [\textit{Robot perceives the Astronaut's request as on “open planning” (as
  oppose to specific executive) task delegation.}]\end{varwidth}}\\
  
  \item \textit{\textbf{C3. Astronaut:}} I will try to fix it asap.\\

  \item \textit{\textbf{C4. Robot:}} I might need to ask some questions while I
  am installing the second panel.\\

  \item \textit{\textbf{C5. Astronaut:}} That's fine. Just let me know.
  
\end{description}
}
\end{spacing}

\subsection{Delegation of a Task (Emotion-Ignorance)}
\label{sec:exp4}

This last hypothetical example is also about task delegation (similar to the
example in Section \ref{sec:exp3}) and it shows how ignoring the collaborator's
emotions in task delegation procedure can negatively impact the progress of a
collaboration. In this example, the emotion-ignorant Robot is doing planning in
its most efficient manner (efficient because time is short); asking a lot of
questions (see utterances in D2, D4, D6, D8, D10) so that it can work out the
plan. But asking questions exacerbates the Astronaut's worry which leads to an
unsuccessful collaboration due to the lack of time. 

As it is shown, the very first utterance of the Astronaut (D1) is the same as
the first utterance in previous example (C1). The Astronaut is worried and
expresses her worry. However, the Robot is not capable of perceiving and
consequently acknowledging the Astronaut's emotion. As a result, the Robot
responds to the Astronaut while the Astronaut's proposed task changes the
Robot's focus of attention to a new task, and now, the Robot tries to determine
a proper solution for an action selection problem. The reason that the Robot
perceives the Astronaut's proposal as an action selection problem is primarily
caused by the shift in the Robot's focus of attention from an unfinished ongoing
task (unsatisfied postconditions) to a new partially known nonprimitive task
(i.e., installing the second panel). Therefore, the Robot immediately tries to
confirm leaving the current unfinished task (D2). Notice the absence of
acknowledging the Astronaut's emotion by the Robot in this turn (compare C2 in
Section \ref{sec:exp3} and D2 here). This vacancy of the emotional awareness is
the beginning of the failure of the task delegaton process. As we can see, the
Robot's response does not mitigate the Astronaut's worriedness about the future
of the collaboration.\footnote{The underlined section in D2 shows the Robot's
need for confirmation of leaving an unfinished task.} Next, the Astronaut tries
to help the Robot to select the proper action by responsing possitively about
the Robot leaving the current task (D3). Now, the Robot shifts its focus of
attention to the new task and uses the Collaboration mechanism (see Section
\ref{sec:AMCT}) to obtain required information such as task dependencies,
existing preconditions and required resources as well as the existance of a
plan. Subsequently, the absence of the required information and dependencies to
the Astronaut's actions causes the Robot to ask a question about whether the
Astronaut will be able to help in some parts of the task (D4). Although, the
Robot's interactive behavior is crucial in many collaborative contexts, here it
implies that the Astronaut's worriedness caused by the lack of time is ignored
by the Robot. This ignorance of the Astronaut's emotional state influences her
mental states and consequently her behavior. Thus, she succinctly responds to
the Robot's question while she is still worried about finishing of installing
the first panel (D5). The Robot asks another question about the required inputs
for the task which are dependent to the Astronaut without considering her
worriedness (D6). At this point, since the Astronaut believes that the Robot's
questions are unneccessary, she becomes frustrated and impatiently answers the
Robot's question (D7). However, once again, not only the Robot misses the
Astronaut's emotion, but it also uses a collaboration function (i.e., Reflection
- see Section \ref{sec:AMCT}) to prevent failure of a task in the future based
on reflection of the same task in similar situation in the past
(D8).\footnote{Notice that the undelined section in D8 is the result of the
Robot's inference about the possibility of a future problem.} Then, the
Astronaut does not recall the event referenced by the Robot and since she is
frustrated, she does not even try to remove the ambiguity of the existing issue
(D9). Therefore, once again, the Robot misses the Astronaut's frustration and
tries to see whether the Astronaut wants the Robot to clarify the issue for her
by providing her some examples (D10).\footnote{The underlined utterance in D10
indicates another situation in which the Robot misses the Astronaut's emotion.}
At last, the Astronaut terminates the collaboration task because of the lack of
time (D11).\\

\begin{spacing}{1.2}
\small{
\begin{description}
  \item \textit{\textbf{D1. Astronaut:}} I still have some problems with
  attaching the first panel! We do not have enough time. You should begin to
  install the second panel.\\

  \item \fbox{\begin{varwidth}{0.96\textwidth} \textit{\textbf{D2. Robot:}}
  Okay. I can handle that. \underline{Is it okay if I leave the current task
  though?}\end{varwidth}}\\
  
  \item \textit{\textbf{D3. Astronaut:}} Yes, that's fine. I can finish it by my
  own.\\

  \item \textit{\textbf{D4. Robot:}} Okay. But according to the plan, I need
  your help when we need to weld the panel. Will you be able to help me?\\

  \item \textit{\textbf{D5. Astronaut:}} Yes, I will.\\
  
  \item \textit{\textbf{D6. Robot:}} We also need your welding tool and supplies
  as well as the measuring tool to install the second panel.\\
  
  \item \textit{\textbf{D7. Astronaut:}} Just let me know whenever you need
  something!\\
  
  \item \fbox{\begin{varwidth}{0.96\textwidth} \textit{\textbf{D8. Robot:}}
  Thanks. I will. \underline{How about the conflict that might occur while I am
  trying} \underline{to fix the second panel?} You are going to need my help at
  the same time.\end{varwidth}}\\
  
  \item \textit{\textbf{D9. Astronaut:}} Robot, I really don't understand what
  you are talking about!\\
  
  \item \fbox{\begin{varwidth}{0.96\textwidth} \textit{\textbf{D10. Robot:}}
  \underline{Do you want me to provide some examples?} \end{varwidth}}\\

  \item \textit{\textbf{D11. Astronaut:}} We don't have time for this anymore!
  
\end{description}
}
\end{spacing}

\section{Computational Framework}
\label{sec:computational-framework}

As we mentioned in Section \ref{sec:example-scenario}, all provided examples in
Sections \ref{sec:exp1} to \ref{sec:exp4} will be discussed in more detail using
a computational theory which we call it the \textit{Affective Motivational
Collaboration Theory}. There are several mechanisms involved in our
computational framework applying different concepts of the \textit{Affective
Motivational Collaboration Theory} to provide an emotion-regulated collaborative
behavior for a robot. In this section, we are going to breifly describe all
these mechanisms without elaborating the constructing processes involved in
each mechanism since it is not in the scope of this paper. We also explain
different types of associated mental states in our computational framework based
on the \textit{Affective Motivational Collaboration Theory}.

\subsection{Affective Motivational Collaboration Theory}
\label{sec:AMCT}

\textit{Affective Motivational Collaboration Theory} is about the interpretation
and prediction of the observable behaviors in a dyadic collaborative
interaction. The theory focuses on the processes regulated by emotional states.
It aims to explain both rapid emotional reactions to events as well as
slower, more deliberative responses. These observable behaviors represent the
outcome of reactive and deliberative processes related to the interpretation of
the self's relationship to the collaborative environment. These reactive and
deliberative processes are triggered by two types of events: \textit{external}
events, such as the other's \textit{utterances} and \textit{primitive actions},
and \textit{internal} events, comprising changes in the self's mental states,
such as belief formation and emotional changes. \textit{Affective Motivational
Collaboration Theory} explains how emotions regulate the underlying processes in
the occurrence of these events during collaboration.

Emotion-regulated processes operate based on the self's mental states including
the anticipated mental states of the other, generated according to the self's
model of the other. These mental states include beliefs, intentions, goals,
motives and emotion instances. Each of these mental states possesses multiple
attributes impacting the relation between cognition and behavior or perception.

\begin{figure}[h!]
  \includegraphics[scale=0.78]{figure/theory-general-croped.pdf}
  \caption{Roadmap of \textit{Affective Motivational Collaboration Theory}
  showing primary influences between processes.}
  \label{fig:theory}
\end{figure}

There are several theories which describe the underlying structure of a
collaboration based on mental states of the collaborators. The collaboration
structure of \textit{Affective Motivational Collaboration Theory} is based on
the SharedPlans theory \cite{grosz:shared-plans}. \textit{Affective Motivational
Collaboration Theory} focuses on the processes that generate, maintain and
update this structure based on mental states. The collaboration structure is
important because social robots ultimately need to co-exist with humans, and
therefore need to consider humans mental states as well as their own internal
states and operational goals.

\subsection{Underlying Mechanisms}
\label{sec:mechanisms}

The \textit{Affective Motivational Collaboration Model} consists of seven
mechanisms (see Fig.~\ref{fig:theory}) most of which directly store and
fetch the data in the Mental States.

\subsubsection{Collaboration Mechanism}
\label{sec:collaboration-mech}

The \textit{Collaboration} mechanism (see Fig.~\ref{fig:theory}) maintains
constraints on actions. These constraints include constraints on task states and
on the ordering of tasks. The Collaboration mechanism also provides processes to
update and monitor the shared plan. These processes depend on the Appraisal
mechanism to evaluate the current Mental States with respect to the current
status of the collaboration. The self also shifts its focus of attention
according to the outcome of the Appraisal mechanism. Moreover, the Collaboration
mechanism can help the self to identify the failure of a task. The Appraisal and
Motivation mechanisms provide interpretation of task failure and the formation
of new Mental States (e.g.\,intentions) respectively. Ultimately, the Coping
mechanism allows the self to perform behavior appropriate to the current state
of the collaboration.

\subsubsection{Appraisal \& Coping Mechanism}
\label{sec:appraisal-coping-mech}

The \textit{Appraisal \& Coping} mechanism (see Fig.~\ref{fig:theory}) consists
of the two processes of Appraisal and Coping. The Appraisal mechanism is
responsible for evaluating changes in the self's Mental States, the anticipated
Mental States of the other, and the state of the collaboration environment.
Consequently, the Appraisal mechanism (Fig.~\ref{fig:theory}) is connected to a)
the Theory of Mind mechanism, to serve as an evaluator whenever the self applies
the Appraisal mechanism in reverse appraisal \cite{gratch:reverse-appraisal}, b)
the Collaboration mechanism, to interpret the progress and changes in the
collaboration plan and associated Mental States, c) the Motivation mechanism, to
generate and assess the self's new goal-driven motives whenever a new motive or
intention is required, e.g., following the failure of a task, and d) the
Perception mechanism, to interpret the external events from the collaboration
environment. The Coping mechanism provides the self with different coping
strategies associated with changes in the self's mental states with respect to
the state of the collaboration. In other words, the Coping mechanism produces
cognitive responses based on the appraisal patterns.

\subsubsection{Motivation Mechanism}
\label{sec:motivation-mech}

The \textit{Motivation} mechanism (see Fig.~\ref{fig:theory}) operates whenever
the self a) requires a new motive to overcome an internal impasse in an ongoing
task, or b) wants to provide an external motive to the other when the other
faces a problem in a task. In both cases, the Motivation mechanism uses the
Appraisal mechanism to compute attributes of the competing motives. Also, the
Motivation mechanism can serve the Theory of Mind mechanism by helping the self
to infer the motive behind the other's current action. Moreover, if there is an
impasse in accomplishing a collaborative task, the self requires a new intention
to take a new action no matter whether the self or the other is responsible for
the task. In this case, the Motivation mechanism applies the beliefs associated
with the blocked task as well as the Appraisal mechanism to generate and compare
a new set of motives related to the status of the collaboration. Only one of
these competing motives is most likely to become a new intention. The Motivation
mechanism forms a new belief and ultimately a new intention based on the winning
motive. As a result, the self can take an action based on the new intention to
sustain the collaboration progress.

\subsubsection{Theory of Mind Mechanism}
\label{sec:tom-mech}

The \textit{Theory of Mind} mechanism (see Fig.~\ref{fig:theory}) is the
mechanism of inferring a model of the other's anticipated Mental States. The
self will progressively update this model during the collaboration. The
refinement of this model helps the self to anticipate the other's mental state
more accurately, which ultimately impacts the quality of the collaboration and
the achievement of the shared goal. Furthermore, the self can make inferences
about the motive (or intention) behind the other's actions using the Motivation
mechanism. This inference helps the self to update its own beliefs about the
other's mental state. In the reverse appraisal process, the self also applies
the Appraisal mechanism together with updated beliefs about the other's Mental
States to make inferences about the other's current mental state based on the
other's emotional expression. Finally, the Collaboration mechanism provides the
collaboration structure, including status of the shared plan with respect to the
shared goal and the mutual beliefs to the Theory of Mind mechanism.
Consequently, any change to the self's model of the other will update the self's
mental state.

\subsection{Mental States \& Emotion Instances}

The Mental States shown in Figure \ref{fig:theory} comprise the knowledge base
required for all the mechanisms in the overall model.

\subsubsection{Beliefs}
\label{sec:beliefs}

\textit{Beliefs} are a crucial part of the Mental States. I have two different
perspectives on categorization of beliefs. In one perspective, I categorize
beliefs based on whether they are shared or not between the collaborators. The
SharedPlans \cite{grosz:plans-discourse} theory is the foundation of this
categorization in which for any given proposition the agent may have: a) private
beliefs (the agent believes the human does not know these), b) the inferred
beliefs of the human (the agent believes the human collaborator has these
beliefs), and c) mutual beliefs (the agent believes both the self and the human
have these same beliefs and both of them believe that). From another
perspective, I categorize beliefs based on who or what they are about. In this
categorization, beliefs can be about the self, the other, or they can be about
the environment. Beliefs about the environment can be about internal events,
such as outcomes of a new appraisal or a new motivate, or external events such
as the human's offer, question or request, and general beliefs about the
environment in which the agent is situated. Beliefs can be created and updated
by different processes. They also affect how these processes function as time
passes.

\subsubsection{Intentions}
\label{sec:intentions}

\textit{Intentions} are mental constructs directed at future actions. They play
an essential role in: a) taking actions according to the collaboration plan, b)
coordination of actions with human collaborator, c) formation of beliefs about
self and anticipated beliefs about the other, and d) behavior selection in the
Coping mechanism. First, taking actions means that the agent will intend to take
an action for primitive tasks that have gained the focus of attention, possess
active motives, have satisfied preconditions for which required temporal
predecessors have been successfully achieved. Second, intentions are involved
in action coordinations in which the human's behavior guides the agent to infer
an anticipated behavior of the human. Third, intentions play a role in belief
formation mainly as a result of the permanence and commitment inherent to
intentions in subsequent processes, e.g., appraisal of the human's reaction to
the current action and self regulation. And lastly, intentions are involved in
selecting intention-related strategies, e.g., planning, seeking instrumental
support and procrastination, which these strategies are an essential category of
the strategies in the Coping mechanism. Intentions possess a set of attributes,
e.g. \textit{Involvement, Certainty, Ambivalence} which moderate the consistency
between intention and behavior. The issue of consistency between the intentions
(in collaboration) and the behaviors (as a result of the Coping mechanism in the
appraisal cycle) is important because neither of these two mechanisms alone
provides solution for this concern.

\subsubsection{Motives}
\label{sec:motives}

\textit{Motives} are mental constructs which can initiate, direct and maintain
goal-directed behaviors. They are created by the emotion-regulated Motivation
mechanism. Motives can cause the formation of a new intention for the agent
according to: a) its own emotional states (how the agent feels about something),
b) its own private goal (how an action helps the agent to make progress), c) the
collaboration goal (how an action helps to achieve the shared goal), and d)
other's anticipated beliefs (how an action helps the other). Motives also
possess a set of attributes, e.g., \textit{Insistence} or \textit{Failure
Disruptiveness}. These attributes are involved in comparison of newly generated
motives based on the current state of the collaboration. Ultimately, the agent
forms or updates a belief about the winning motive in the Mental States.

\subsubsection{Goals}
\label{sec:goals}

\textit{Goals} help the agent to create and update its collaboration plan
according to the current private and shared goal content and structure, i.e.,
the \textit{Specificity, Proximity} and \textit{Difficulty} of the goal. Goals
direct the formation of intentions to take appropriate corresponding actions
during collaboration. Goals also drive the Motivation mechanism to generate
required motive(s) in uncertain or ambiguous situations, e.g., to minimize the
risk of impasse or to reprioritize goals. The \textit{Specificity} of goals has
two functions for the agent. First, it defines the performance standard for
evaluating the progress and quality of the collaboration. Second, it serves the
agent to infer the winner of competing motives. The \textit{Proximity} of goals
distinguishes goals according to how ``far'' they are from the ongoing task.
Proximal (or short-term) goals are achievable more quickly, and result in higher
motivation and better self-regulation than more temporally distant (or
long-term) goals. Goals can influence the \textit{Strength} of beliefs, which is
an important attribute for regulating the elicitation of social emotions. The
\textit{Difficulty} of goals impacts collaborative events and decisions in the
appraisal, reverse appraisal, motive generation and intention formation
processes. For instance, overly easy goals do not motivate; neither are people
motivated to attempt what they believe are impossible goals.

\subsubsection{Emotions}

\textit{Emotions} in Mental States are emotion instances that are elicited by
the Appraisal mechanism. The agent also keeps beliefs about these emotion
instances in the Mental States. The Belief Formation mechanism creates or
updates these beliefs about emotions. These emotion instances include the
agent's own emotions as well as the anticipated emotions of the other which are
created with the help of the processes in the Theory of Mind mechanism.

\section{Walk Through Computational Examples}

In this section, we are going to discuss how the individual computational
mechanisms (see Section \ref{sec:mechanisms}) are involved in generating the
Robot's collaborative behaviors discussed in each example in Section
\ref{sec:example-scenario}. The following four walkthrough examples are in the
same order as the four examples in Section \ref{sec:example-scenario}. These
examples can demonstrate the applicability of the \textit{Affective Motivational
Collaboration Theory} in modelling and understanding of the emotion-regulated
underlying processes of a collaboration procedure. Notice that in emotional
ignorance examples (Sections \ref{sec:wt-exp2} and \ref{sec:wt-exp4}), we use
the same mechanisms as in the emotional awareness examples.

Also, the utterances in our examples in Section \ref{sec:example-scenario} are
repeated here inside of boxes to help better understanding of the associated
details in our walkthroughs. The walkthrough explanations between these boxes
use a simple and straightforward representation to show how the underlying
mechanisms (see Section \ref{sec:computational-framework}) are invloved to
generate collaborative responses for the Robot.

\subsection{Agreeing on Shared Goal (Emotion-Awareness)}
\label{sec:wt-exp1}

This Section provides a step-by-step walkthrough explanation of the same
example presented in Section \ref{sec:exp1}. In this example, the description
between Astronaut's utterance A1 and Robot's utterance A2 explains how the
collaboration function of \textit{awareness} works to perceive and interpret
events including other's utterances and emotional expressions, and how the
collaboration function of \textit{engagement} works to help the Robot to take
appropriate action whenever it is required.\footnote{Since our walkthrough
explanation of underlying processes is based on collaborators' utterances, we
use \textbf{verbal} expression of emotions within the utterances to emphasize
their existance in certain parts of the collaboration. However, although the
nonverbal emotional expressions (e.g., facial expressions) can provide the same
impact during collaboration, the automatic recognition of them is out of our
research context.}\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}
\textit{\textbf{A1. Astronaut:}} Oh no! Finishing the quality check of our
installation with this measurement problem is so frustrating. I think we should
stop now!\end{varwidth}}\\ \\

%\fontsize{9pt}{10pt}\selectfont
\noindent\item \textbf{(Perception)} Robot perceives Astronaut's utterances and
emotion.\\
  
First, the Robot perceives the Astronaut's utterances as well as her emotion in
the first turn, i.e., A1. The perception mechanism (see Section \ref{sec:AMCT})
forms beliefs about the task under the Astronaut's focus of attention, and also
the Astronaut's emotion which she has expressed both verbally and nonverbally.
The beliefs formed about the task (i.e., installing the panel) include:

\begin{itemize}
  \item[$\bullet$] the Astronaut's proposal of \textit{stopping} the task,
  \item[$\bullet$] which is a \textit{future} event,
  \item[$\bullet$] and is \textit{caused by} the measurement tool problem.
\end{itemize}

\noindent Also, beliefs formed about the Astronaut's emotion (i.e., frustration)
include:

\begin{itemize}
  \item[$\bullet$] the existence of a \textit{negative valenced} emotion,
  \item[$\bullet$] which maps into a three-value vector of \textit{pleasure},
  \textit{arousal}, and \textit{dominance},
  \item[$\bullet$] and is verbally conveyed as \textit{frustration}.\\
\end{itemize}

\noindent\textbf{(Collaboration: \textit{Monitoring \& Focus Shifting})} Robot
uses the Collaboration mechanism (see Section \ref{sec:collaboration-mech}) to
form new beliefs about the collaboration status based on its perception. These
new beliefs are about:

\begin{itemize}
  \item[$\bullet$] the \textit{unsatisfied} precondition of the Astronaut's
  current \textit{task},
  \item[$\bullet$] the \textit{blocked} status of the Astronaut's current
  \textit{task},
  \item[$\bullet$] and consequently the \textit{blocked} status of the
  \textit{shared goal},
  \item[$\bullet$] which causes the change in the Robot's \textit{focus of
  attention} to the Astronaut's task.
\end{itemize}

\noindent\item \textbf{(Theory of Mind: \textit{Reverse Appraisal \& User
Modeling})} Robot uses reverse appraisal to understand the meaning of
Astronaut's frustration according to the collaborative task status (e.g.,
precondition and shared goal status). Robot updates Astronaut's user model
respectively.\\

The reverse appraisal process (see Section \ref{sec:appraisal-coping-mech})
forms beliefs about the anticipated appraisals of the Astronaut with respect to
the current task's status based on the Astronaut's utterances and emotion in A1,
and the output of the collaboration mechanism. Some of these anticipated
appraisal values indicate that the event is interpreted as \textit{relevant},
\textit{undesirable}, \textit{uncontrollable}, \textit{urgent}, and
\textit{unexpected} by the Astronaut. Furthermore, the user modeling process
updates the Astronaut's user model based on the output of the reverse appraisal
process and the collaboration mechanism; this user modeling process forms
beliefs that a) Astronaut has \textit{low autonomy}, and b) Astronaut is a
\textit{highly communicative} collaborator.\\

\noindent\item \textbf{(Appraisal)} Robot appraises Astronaut's utterances and
emotion.\\

The Appraisal mechanism simultaneously uses distinct processes to compute values
for each individual appraisal variable. The output of these processes provides a
vector of values describing the Robot's interpretation of the current event,
(A1). The outcome will be mapped to a particular emotion instance, i.e.,
worriedness, since the Astronaut is frustrated and the task is blocked which
makes the Robot to properly express an appropriate emotion as a response to the
Astronaut's emotional state. The robot can choose to express this emotion
verbally or nonverbally as part of its communication if it is required.\\

\noindent\item \textbf{(Motivation: \textit{Motive \& Intention Formation})}
Robot forms new motives according to the result of:

\begin{enumerate}[a)]
  \item appraisal with respect to the shared goal,
  \item reverse appraisal of the Astronaut's emotion,
  \item and the user model of the Astronaut. 
\end{enumerate}

Then, the motive comparator process compares current available motives and
sorts them based on their distance to the Astronaut's emotional state and
the achievement of the shared goal.\footnote{Here, the distance function is a
function of a) the Astronaut's emotional state as an admissible approximation
of her mental state, and b) how taking an action based on the corresponding
intention of a particular motive improves the possibility of the collaborators
reaching to the collaborators' mutually accepted shared goal.} The Robot,
ultimately, selects the most related motive and forms a new intention with
respect to the current status of collaboration. After this whole process, the
\textit{engagement} function of collaboration uses the coping mechanism to take
an action based on the available intention.\\

\noindent\item \textbf{(Coping)} Based on the current mental state, the Robot
chooses an emotion-focused coping strategy (see Section
\ref{sec:appraisal-coping-mech}) and decides to acknowledge Astronaut's emotion,
and provide an alternative solution. Subsequently, the Robot responds to the
Astronaut with appropriate utterances appearing in A2.\\

\noindent \fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{A2. Robot:}}
I see. This is frustrating. But, I can help you with the measurement
tool and we can finish the task as originally planned. \end{varwidth}}\\

The Astronaut's new utterance (A3) provides the Robot with a new question about
whether the Robot can fix the measurement tool. The followings show how the
robot use collaboration function of \textit{mediation} to employ the same
mechanisms described in Section \ref{sec:computational-framework}. This
collaboration function enables the Robot to negotiate with the Astronaut to
reach an agreement on the shared goal.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{A3.
Astronaut:}} Can you fix the measurement tool?\end{varwidth}}\\

Notice that the processes in the awareness function will be run again (similar
to what we had above) for the new utterance of the Astronaut (A3) before the
following processes are run. The new beliefs as the output of the awareness
function based on the Astronaut's new utterance are:

\begin{itemize}
  \item[$\bullet$] the precondition associated to the Astronaut's current
  \textit{task} is still \textit{unsatisfied},
  \item[$\bullet$] the status of the Astronaut's current \textit{task} is still
  \textit{blocked},
  \item[$\bullet$] and similarly the status of the \textit{shared goal} is
  still \textit{blocked},
  \item[$\bullet$] however, the Astronaut's question changes the Robot's
  \textit{focus of attention} to the measurement tool,
  \item[$\bullet$] also, the Astronaut's \textit{emotion} has changed to
  \textit{neutral}.
  \item[$\bullet$] but, her user model \textit{stays the same}, i.e.,
  having low-autonomy and being highly communicative.
\end{itemize}

%\begin{spacing}{1.15}
%\fontsize{9pt}{10pt}\selectfont
\noindent\textbf{(Collaboration)} The change in the focus of attention to the
measurement tool causes the Robot to check the availability of a recipe to fix
or replace the disfunctioning measurement tool. Robot finds a recipe to replace
the measurement tool.\\

\noindent\textbf{(Appraisal)} Robot appraises the possibility of
replacing the measurement tool with respect to: a) the status of the shared
goal, and b) the Astronaut's user model. Robot finds the replacement of the
measurement tool \textit{relevant}, \textit{desirable}, and
\textit{controllable}.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms new motives based on the next task according to the shared plan and
outcome of the appraisal of the possibility of replacing the measurement tool.
Robot forms the corresponding intentions with respect to the new motives.\\

Once again, the Robot uses the \textit{engagement} function of collaboration to
take an action based on the recent intention.\\

\noindent\textbf{(Coping)} Based on the current mental states, Robot
chooses to negotiate and offer an alternative action to the Astronaut. (A4)\\
%\end{spacing}

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{A4. Robot:}} The
next task is fixing the panel and it needs you to prepare and attach the welding
rod to your welding tool. To save our time, I will fetch another measurement
tool while you are preparing your welding tool.\end{varwidth}}\\

At this point, Astronaut is content with the way Robot outlined the shared goal
and responds respectively (A5). Robot uses the same awareness function as we
discussed above, therefore, Robot percieves and interprets the Astronauts
response as an agreement on their new shared goal.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{A5. Astronaut:}}
That would be great!\end{varwidth}}

\subsection{Agreeing on Shared Goal (Emotion-Ignorance)}
\label{sec:wt-exp2}

This walkthrough example begins with the same exact utterance as the previous
one, and it provides the corresponding details of the emotion-ignorant example
in Section \ref{sec:exp2}. To avoid redundant explanations, we refer to similar
procedures in previous walkthrough example.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B1. Astronaut:}}
Oh no! Finishing the quality check of our installation with this measurement
problem is so frustrating. I think we should stop now!\end{varwidth}}\\

\noindent\textbf{(Perception)} Robot only perceives Astronaut's utterances.
(B1)\\

Here, in the first step, Robot perceives the Astronaut's utterances and ignores
her expressed emotion in B1, i.e., frustration. Similarly to previous
example, the perception mechanism forms beliefs about the task under the
Astronaut's focus of attention. These beliefs include:

\begin{itemize}
  \item[$\bullet$] the Astronaut's proposal of \textit{stopping} the task,
  \item[$\bullet$] which is a \textit{future} event,
  \item[$\bullet$] and is \textit{caused by} the measurement tool problem.
\end{itemize}

\noindent Notice that beliefs about the Astronaut's emotion are formed
differently in compare to previous example, and they are based on neutral
emotion of the Astronaut, since the Robot ignores the Astronaut's actual
emotion instance, i.e., frustration.\footnote{In emotion-ignorant examples, we
assume Robot always perceives neutral emotion expressed by the Astronaut.}

\begin{itemize}
  \item[$\bullet$] the existence of a \textit{neutral valenced} emotion,
  \item[$\bullet$] which maps into a three-value vector of \textit{pleasure},
  \textit{arousal}, and \textit{dominance},
  \item[$\bullet$] and is verbally conveyed as \textit{neutral} emotion.\\
\end{itemize}

\noindent\textbf{(Collaboration)} Robot uses the collaboration mechanism to form
new beliefs about the collaboration status based on its perception. These new
beliefs are the same as the previous example:

\begin{itemize}
  \item[$\bullet$] the \textit{unsatisfied} precondition of the Astronaut's
  current \textit{task},
  \item[$\bullet$] the \textit{blocked} status of the Astronaut's current
  \textit{task},
  \item[$\bullet$] and consequently the \textit{blocked} status of the
  \textit{shared goal},
  \item[$\bullet$] which causes the change in the Robot's \textit{focus of
  attention} to the Astronaut's task.
\end{itemize}

\noindent\textbf{(Theory of Mind: \textit{Reverse Appraisal \& User Modeling})}
Robot uses reverse appraisal to understand the meaning of Astronaut's neutral
emotion according to the collaborative task status (e.g., precondition and
shared goal status). Robot updates Astronaut's user model respectively.\\

The reverse appraisal process, as we discussed in previous example, forms
beliefs about the anticipated appraisals of the Astronaut with respect to the
current task's status based on the Astronaut's utterances and emotion that the
Robot perceives, as well as the output of the collaboration mechanism. In this
example, since the Robot misses the actual expressed emotion by the Astronaut
(i.e., frustration) and perceives her with amiss neutral emotion, the
corresponding anticipated appraisal values indicate wrong interpretation of the
event. For instance, the output of the reverse appraisal mechanism could
indicate a \textit{relevant}, \textit{desirable}, \textit{controllable},
\textit{non-urgent}, and \textit{expected} interpretation of the current event
by the Astronaut. Furthermore, the user modeling process updates the Astronaut's
user model based on the output of the reverse appraisal process and the
collaboration mechanism; this user modeling process forms beliefs that a)
Astronaut has \textit{high autonomy}, and b) Astronaut is a
\textit{moderately communicative} collaborator.\\

\noindent\textbf{(Appraisal)} Robot appraises Astronaut's utterances.\\

The Appraisal mechanism operates similar to what we discussed in Section
\ref{sec:exp1}. The output of these processes provides a vector of values
describing the Robot's interpretation of the current event (B1). The outcome
will be also mapped to a particular emotion instance, but since the Robot misses
Astronaut's emotion, it maps the appraisals to a defferent emotion, i.e.,
hope, than the one elicited in previous example. Robot elicits hope because it
believes the Astronaut's emotion is neutral and the current task is blocked.
Therefore, the Robot wants to immediately come up with an alternative
solution.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formation})} As
we discussed earlier, Robot forms new motives according to the result of:

\begin{enumerate}[a)]
  \item appraisal with respect to the shared goal,
  \item reverse appraisal of the Astronaut's emotion,
  \item and the user model of the Astronaut. 
\end{enumerate} 

Although the process of comparing and sorting available motives is similar to
previous example, all of the new motives are different than the motives in that
example. The reason is that each of the above three sources of motives forms a
different motive because of holding different value which is caused by the
ignorance of the Astronaut's actual emotion. For instance, the motive generated
with the influence of appraisal in emotional-awareness example urges the Robot
to postpone asking questions about the alternative solutions while the motive
with the same cause (i.e., appraisal) in emotional-ignorance example urges the
Robot to immediately try to fix the problem and come up with alternative
solutions by asking questions. The Robot, similar to previous example, selects
the most related motive and forms a new intention with respect to the current
status of collaboration. After this whole process, the Robot uses the coping
mechanism to take an action based on the available intention.\\

\noindent\textbf{(Coping)} Based on the current mental states, Robot decides to
use problem-focused coping strategy of seeking information to be able to choose
between two available actions and reduce the current amount of uncertainty.
Therefore, the Robot, without acknowledging the Astronaut's emotion, asks the
Astronaut to choose between two alternative solutions. (B2)\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{B2. Robot:}} I
can help you with the measurement tool, or we can terminate this task.
What do you want me to do?\end{varwidth}}\\

As we mentioned earlier in Section \ref{sec:exp2}, Robot's response does not
make any progress in collaboration status. Hence, Astronaut repeats herself
about the task status (B3).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{B3. Astronaut:}}
As I said the measurement tool does not work properly. We can not continue!
\end{varwidth}}\\

Robot perceives Astronaut's new utterance (B3) while, again, ignores her
frustration. Robot goes through the same process as we mentioned above, and
since Astronaut has just repeated herself, her new utterances do not change the
Robot's mental state. Having the same mental state causes the Robot to ask
similar question (B4).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{B4. Robot:}}
Okay. Do you want me to fix this problem or terminate the task?\end{varwidth}}\\

This time, Robot's question makes an ambiguous assumption for the Astronaut on
whether the Robot can fix the disfunctional measurement tool for her. The
ambiguity of Robot's question does not help Astronaut's frustration and causes
her to ask a clarification question (B5).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B5. Astronaut:}}
Can you fix my measurement tool?\end{varwidth}}\\

Again, the same processes will be run again (similar to what we had above) for
the new utterance of the Astronaut (B5) before the following process. The new
beliefs as the output of the awareness function based on the Astronaut's new
utterance are as it follows. Notice that the Robot still believes that the
Astronaut's emotion is neutral.

\begin{itemize}
  \item[$\bullet$] the precondition associated to the Astronaut's current
  \textit{task} is still \textit{unsatisfied},
  \item[$\bullet$] the status of the Astronaut's current \textit{task} is still
  \textit{blocked},
  \item[$\bullet$] and similarly the status of the \textit{shared goal} is
  still \textit{blocked},
  \item[$\bullet$] however, the Astronaut's question changes the Robot's
  \textit{focus of attention} to fixing the measurement tool,
  \item[$\bullet$] also, the Astronaut's \textit{emotion} is still
  \textit{neutral}.
  \item[$\bullet$] but, her user model \textit{has changed} to having
  medium-autonomy and being highly communicative.
\end{itemize}

%\begin{spacing}{1.15}
%\fontsize{9pt}{10pt}\selectfont
\noindent\textbf{(Collaboration)} The change in the focus of attention to fixing
the measurement tool causes the Robot to check the availability of a recipe to
fix or replace the disfunctioning measurement tool. Similar to previous
example, Robot finds a recipe to replace the measurement tool.\\

\noindent\textbf{(Appraisal)} Robot appraises the possibility of
replacing the measurement tool with respect to: a) the status of the shared
goal, and b) the Astronaut's user model. Robot finds the replacement of the
measurement tool \textit{relevant}, \textit{desirable}, and
\textit{controllable} just as before.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms new motives based on the next task according to the shared plan and
outcome of the appraisal of the possibility of replacing the measurement tool.
Robot forms the corresponding intentions with respect to the new motives.\\

Once again, the Robot decides to take an action based on the recent intention.\\

\noindent\textbf{(Coping)} Based on the current mental states, first, Robot
responds to the Astronaut's question, and then, chooses to negotiate and offer
an alternative action to the Astronaut(B6).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B6. Robot:}} I
cannot fix your measurement tool, but I can fetch another one for you if you
want?\end{varwidth}}\\

Astronaut's strong emotion, shortage of time, and Robot's mismatching answer to
Astronaut's assumption causes the Astronaut to reject the Robot's proposal
(B7).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B7. Astronaut:}}
No, I don't want another measurement tool! We don't have time for
that!\end{varwidth}}\\

After perceiving Astronaut's answer Robot tries to negotiate (using the same
procedure as we discussed above) with the Astronaut to protect the collaboration
and the shared goal from failure. Therefore, the Robot asks about the
possibility of pusuing another task.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B8. Robot:}}
Okay. You want me to terminate this task. Terminating this task can influence
the quality of installing this solar panel which can cause the mission to fail.
Or, do you want us to work on another task? This can help us to install the
panel using your welding tool, but I do not know whether the quality of our
installation will be acceptable.\end{varwidth}}\\

Astronaut terminate the collaboration due to the lack of time and failure in
Robot's collaborative behavior (B9).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B9. Astronaut:}}
I told you we have this problem and we should terminate the mission! We cannot
continue without the measurement tool!\end{varwidth}}\\

As it was shown in this example, ignoring Astronaut's emotion, first, impacts
the Robot's perception and corrresponding beliefs. The output of collaboration
mechanism remains unchanged in compared to the emotional-awareness example which
is a crucial point in our first two examples. Although the collaboration
mechanism provides the required structural details of collaboration between the
Robot and the Astronaut, these structural details are not enough for saving a
collaboration from a failure. As we continue, we can see ignoring the actual
emotion of the Astronaut causes misfunctioning of the processes in the Theory of
Mind mechanism, i.e., reverse appraisal and user modeling. Comoparing the result
of these two processes with the results in emotional-awareness example, shows
the importance of correctly perceiving a collaborator's emotion. This problem
continues even with appraisal mechanism which maps the Robot's interpretation of
the environment to a wrong emotion. Consequently, all sources of the motivation
mechanism provide incorrect values which drastically influence the formation of
the underlying motives of the required intentions. Finally, the coping mechanism
operates based on wrong newly formed intentions which leads to a totally
different behavior of the Robot in compare to the same turn in
emotional-awareness example. The divergance of the Robot's collaborative
behavior from its successful path contniues among the Robot and the Astronaut's
interaction which increases the required time for achieving the shared goal, and
pepetuates the negative feeling of the Astronaut. The Robot also misses the
right time to begin a negotiation process to save the collaboration from
failure. Therefore, it causes the Astronaut to reject Robot's proposal which
again aggravates the Astronaut's negative emotion. Consequently, the same
collaboration fails even though that the Robot uses the same computational
mechanisms, as we showed above.

\subsection{Delegation of a Task (Emotion-Awareness)}
\label{sec:wt-exp3}

This walkthrough example is focusing on delegation of a task by the Astronaut
during collaboration (see also the example in Section \ref{sec:exp3}). In this
example, the explanation between Astronaut's utterance C1 and Robot's utterance
C2 provides the details of how different mechanisms discussed in Section
\ref{sec:computational-framework} are involved making the Robot to show
collaborative behaviors in acceptance of a new delegated task. To avoid
redundant explanations, we refer to similar procedures in previous walkthrough
examples.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C1. Astronaut:}}
I still have some problems with attaching the first panel! We do not have enough
time. You should begin to install the second panel.\end{varwidth}}\\

\noindent\textbf{(Perception)} Robot perceives Astronaut's utterances and
emotion in C1.\\

The perception mechanism forms beliefs based on the Astronaut's utterances and
her emotion (i.e., worriedness) which she has expressed nonverbally. We have
shown some examples of the beliefs formed by perception mechanism in the example
in Section \ref{sec:wt-exp1}.\\

\noindent\textbf{(Collaboration: \textit{Interruption \& Constraint
Management})} Robot infers the interruption and uses the constraint management
process to retrieve required resources and preconditions. Robot also checks
whether there is an available associated recipe for the delegated task.\\

\noindent\textbf{(Theory of Mind: \textit{Reverse Appraisal \& User Modeling})}
Robot uses reverse appraisal to understand the meaning of the Astronaut's
worriedness with respect to the collaborative task status retrieved in previous
step (e.g., precondition status, postcondition status, required resources,
shared goal). Robot also updates the Astronaut's user model and froms belief
that a) Astronaut has \textit{high autonomy}, and b) Astronaut is a highly
communicative collaborator. We have discussed more details about reverse
appraisal and user modeling processes in our example in Section
\ref{sec:wt-exp1}.\\

\noindent\textbf{(Appraisal)} Robot appraises Astronaut's utterances and
emotion. The Robot interprets the Astronaut's utterances and emotional state as
a \textit{relevant}, \textit{unexpected}, \textit{undesirable}, \textit{urgent},
but \textit{controllable} event.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms new motives based on the result of the same processes we discussed in
Section \ref{sec:wt-exp1}, and compares the available motives in the same way
we discussed in that section. Robot forms new intention(s) with respect to the
selected motive.\\

\noindent\textbf{(Coping)} Based on the current mental states, Robot chooses an
emotion-focused coping strategy and decides to acknowledge Astronaut's emotion,
and provide a proper response (C2) without asking questions about the delegated
task.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{C2. Robot:}}
Okay. Don't worry. I can handle that.\end{varwidth}}\\

Astronaut perceives Robot's acknowledgement of her emotion as well as Robot's
positive response to Astronaut's delegated task. Astronaut knows that the
Robot needs her to help with some of the primitive tasks in her own delegated
task to the Robot. Therefore, the Astronaut, while is still worried about
time, informs the Robot that she will try to quickly finish her current task.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C3. Astronaut:}}
I will try to fix it asap.\end{varwidth}}\\

Robot perceives Astronaut's utterance and emotional expression. The same process
happens from updating beliefs to taking actions as we discussed above or in
previous examples. Since the Robot believes that the Astronaut is still worried
about time, it just informs the Astronaut about some potential questions in the
future. Robot knows about these questions since there are either missing
information according to the partial plan, or required resources and sub-tasks
that can be provided by the Astronaut. Robot chooses a proper utterance about
missing information according to human's emotion (C4).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C4. Robot:}} I
might need to ask some questions while I am installing the second
panel.\end{varwidth}}\\

Astronaut finds the Robot's response appropraite for the delegated task.
Therefore, Robot's proper response mitigates the Astronaut's negative emotion
which was caused by the lack of time for successive installing procedure of the
first and second panels. As a result Astronaut properly responds to the Robot's
needs (C5).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C5. Astronaut:}}
That's fine. Just let me know.\end{varwidth}}

\subsection{Delegation of a Task (Emotion-Ignorance)}
\label{sec:wt-exp4}

This walkthrough example begins with the same exact utterance as the previous
one in Section \ref{sec:wt-exp3}. This section briefly provides the
corresponding details of the emotion-ignorant example in Section \ref{sec:exp4}
which is focusing on delegation of a task by the Astronaut during collaboration.
In this example, the explanation between Astronaut's utterance D1 and Robot's
utterance D2 provides some details to show that even though the same mechanisms
(discussed in Section \ref{sec:computational-framework}) are used to make the
Robot obtaining collaborative behaviors, ignoring the Astronaut's expressed
emotion changes the output of different computational mechanisms (see also
Section \ref{sec:wt-exp2}) which ultimately causes unsuccessful termination of
the task delegation process. To avoid redundant explanations, we group some of
the Astronaut and Robot's utterances which constitute the representation of
dawdling interaction between them. We also refer to similar procedures in
previous walkthrough examples.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D1. Astronaut:}}
I still have some problems with attaching the first panel! We do not have enough
time. You should begin to install the second panel.\end{varwidth}}\\

\noindent\textbf{(Perception)} Robot only perceives Astronaut's utterances
(D1). These beliefs are about unsatisfied postcondition of the first task, i.e.,
installing the first solar panel, and Atronaut's proposal of installing the
second panel. We have shown some examples of the beliefs formed by perception
mechanism in Section \ref{sec:wt-exp1}. Also, as we have shown in Section
\ref{sec:wt-exp2} Robot does not perceive Astronaut's emotion (i.e.,
worriedness). Therefore, Robot misses beliefs about Astronaut's emotion.\\

\noindent\textbf{(Collaboration: \textit{Interruption \& Constraint
Management})} Robot infers the interruption and uses the constraint management
process to retrieve required resources, preconditions. Similar to previous
example, Robot also checks whether there is an available associated recipe for
the delegated task. Ignoring Astronaut's expressed emotion does not change
beliefs formed based on the output of the collaboration mechanism.\\

\noindent\textbf{(Theory of Mind: \textit{Reverse Appraisal \& User Modeling})}
Similar to the example in Section \ref{sec:wt-exp2}, Robot uses reverse
appraisal to understand the meaning of the Astronaut's emotion with respect to
the collaborative task status. However, since the Robot ignores the actual
Astronaut's emotion, the output of the reverse appraisal does not help the
Robot's inference about its own collaborative behavior (see Section
\ref{sec:wt-exp2}). Similarly, Robot updates the Astronaut's user model based on
wrong beliefs achieved by ignoring Astronaut's expressed emotion. We have
discussed more details about reverse appraisal and user modeling processes in
our example in Section \ref{sec:wt-exp1}, and we have shown similar effects
in Section \ref{sec:wt-exp2}.\\

\noindent\textbf{(Appraisal)} Robot appraises Astronaut's utterances with a
wrong assumption of her expressing neutral emotion. Consequently, similar to
the example in Section \ref{sec:wt-exp3}, the Robot interprets the Astronaut's
utterances as a \textit{relevant}, \textit{unexpected}, \textit{undesirable},
\textit{urgent}, and \textit{controllable} event.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms and comopares new motives in the same way as we have discussed in previous
examples. However, both processes of the motive formation and comparision are
performed based on false outputs of the other processes (see also Section
\ref{sec:wt-exp2}). Therefore, Robot forms new intention with respect to the
selected motive which can be formed or selected for a wrong reason.\\

\noindent\textbf{(Coping)} Based on the current mental state (including a
wrong intention), Robot decides to use problem-focused coping strategy of
seeking information to be able to drop the intention of working with the
Astronaut on the same task. Respectively, Robot asks a quesiton (D2).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D2. Robot:}}
Okay. I can handle that. Is it okay if I leave the current task though?
\end{varwidth}}\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D3. Astronaut:}}
Yes, that's fine. I can finish it by my own.\\

\noindent\textbf{(Collaboration , Mental States, Coping)} Based on the current
mental states, Robot decides to use problem-focused coping strategy of planning
to be able to execute the required task by the Astronaut. (D4)\\

\noindent \textit{\textbf{D4. Robot:}} Okay. But according to the plan, I need
your help when we need to weld the panel. Will you be able to help me?\\

\noindent \textit{\textbf{D5. Astronaut:}} Yes, I will.\\
  
\noindent\textbf{(Collaboration , Mental States, Coping)} Based on the current
mental states, Robot decides to use problem-focused coping strategy of seeking
social support (seeking information) to be able to execute the required task by
the Astronaut. (D6)\\

\noindent \textit{\textbf{D6. Robot:}} We also need your welding tool and
supplies as well as the measuring tool to install the second panel.\\
  
\noindent \textit{\textbf{D7. Astronaut:}} Just let me know whenever you need
something! \\
  
\noindent\textbf{(Collaboration , Mental States, Coping)} Based on the current
mental states, Robot decides to use problem-focused coping strategy of active
coping to be able to execute the required task by the Astronaut. (D8)\\

\noindent \textit{\textbf{D8. Robot:}} Thanks. I will. How about the conflict
that might occur while I am trying to fix the second panel? You are going to
need my help at the same time.\\
  
\noindent \textit{\textbf{D9. Astronaut:}} Robot, I really don't understand what
you are talking about! \end{varwidth}}\\
  
\noindent\textbf{(Collaboration , Mental States, Coping)} Based on the current
mental states, Robot decides to use problem-focused coping strategy of planning
to be able to execute the required task by the Astronaut. (D10)\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D10. Robot:}}
Do you want me to provide some examples? \end{varwidth}}\\  
  
\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D11.
Astronaut:}} We don't have time for this anymore! \end{varwidth}}

\section{Related Work}

\subsection{Emotions in Social Context}

\subsection{Social Functions of Emotions}

\subsection{Affect and Motives}

\subsection{Collaboration Theories}
\label{sec:collaboration-theory}

The prominent collaboration theories are mostly based on plans and joint
intentions
\cite{cohen:teamwork,grosz:plans-discourse,Litman:discourse-commonsense}, and
they were derived from the BDI paradigm developed by Bratman
\cite{bratman:intentions-plans} which is fundamentally reliant on folk
psychology \cite{ravenscroft:folk}. The two theories, Joint Intentions
\cite{cohen:teamwork} and SharedPlans \cite{grosz:plans-discourse}, have been
extensively used to examine and describe teamwork and collaboration.

The SharedPlans theory is based on the theories of Bratman and Pollack
\cite{bratman:plans-reasoning,pollack:plan-inference,pollack:plan-mental-attitudes},
who outline a mental-state view of plans in which having a plan is not just
knowing how to do an action, but also having the intention to do the actions
entailed. Bratman's views of intention goes back to the philosophical views of
Anscombe \cite{anscombe:intention} and Casta$\tilde{n}$eda
\cite{castaneda:thinking} about intention. Also, as Grosz and Sidner mention in
\cite{grosz:plans-discourse} the natural segmentation of discourse reflects
intentional behaviors in each segment. These intentions are designated as
Discourse Segment Purposes (DSPs) which are the basic reasons for engaging in
different segments of discourse. DSPs are a natural extension of Gricean
intentions at the utterance level \cite{neale:grice-language}.

Cohen and Levesque also mention that in Joint Intentions theory their view of
intention is primarily future-directed \cite{cohen:intention-commitment} which
makes their view similar to Bratman's theory of intention
\cite{bratman:intention}, contra Searle \cite{searle:collective}.

The SharedPlans model of collaborative action, presented by Grosz and Sidner
\cite{grosz:planning-acting,grosz:collaboration,grosz:plans-discourse}, aims
to provide the theoretical foundations needed for building collaborative
robots/agents \cite{grosz:collaborative-systems}. SharedPlans is a general
theory of collaborative planning that requires no notion of joint intentions,
accommodates multi-level action decomposition hierarchies and allows the process
of expanding and elaborating partial plans into full plans. SharedPlans theory
explains how a group of agents can incrementally form and execute a shared plan
that then guides and coordinates their activity towards the accomplishment of a
shared goal. SharedPlans is rooted in the observation that collaborative plans
are not simply a collection of individual plans, but rather a tight interleaving
of mutual beliefs and intentions of different team members. In
\cite{grosz:collaboration} Grosz and Kraus use first-order logic to present the
formalization of SharedPlans.

Grosz and Sidner in \cite{grosz:plans-discourse} present a model of plans to
account for how agents with partial knowledge collaborate in the construction of
a domain plan. They are interested in the type of plans that underlie discourse
in which the agents are collaborating in order to achieve a shared goal. They
propose that agents are building a shared plan in which participants have a
collection of beliefs and intentions about the actions in the plan. Agents have
a library of how to do their actions, i.e. recipes. These recipes might be
partially specified as to how an action is executed, or contributes to a goal.
Then, each agent communicates their beliefs and intentions by making utterances
about what actions they can contribute to the shared plan. This communication
leads to the construction of a shared plan, and ultimately termination of the
collaboration with each agent mutually believing that there exists one agent who
is going to execute an action in the plan, and the fact that that agent has
intention to perform the action, and that each action in the plan contributes to
the goal \cite{grosz:plans-discourse,lochbaum:plan-models}.

\section{Conclusion and Future Work}

- Talking about other examples that I have.

%\label{sec:2} ~\ref{sec:1}

%\paragraph{Paragraph headings} 


% For one-column wide figures use
%\begin{figure}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
%  \includegraphics{example.eps}
% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:1}       % Give a unique label
%\end{figure}
%
% For two-column wide figures use
%\begin{figure*}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
%  \includegraphics[width=0.75\textwidth]{example.eps}
% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:2}       % Give a unique label
%\end{figure*}
%



% For tables use
%\begin{table}
% table caption is above the table
%\caption{Please write your table caption here}
%\label{tab:1}       % Give a unique label
% For LaTeX tables use
%\begin{tabular}{lll}
%\hline\noalign{\smallskip}
%first & second & third  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%number & number & number \\
%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}


%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)

% Format for books

%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)

% etc
%\end{thebibliography}

\bibliographystyle{abbrv}
\bibliography{mshayganfar.bib}

\end{document}
% end of file template.tex

