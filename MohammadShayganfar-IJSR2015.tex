%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{varwidth}
\usepackage{setspace}
\usepackage{perpage}
\usepackage{enumerate}
\MakePerPage{footnote}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Toward Improving Human-Robot Collaboration with Emotional Awareness}%\thanks{Grants or
% other notes about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
%}

%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Mohammad Shayganfar \and
        Charles Rich \and
        Candace L. Sidner
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Mohammad Shayganfar \and Charles Rich \and Candace L. Sidner \at
              100 Institute Road, Worcester, MA, USA 01609-2280 \\
              Tel.: +1 508-831-5357\\
              Fax: +1 508-831-5776\\
              \email{mshayganfar@wpi.edu}\\
              \email{rich@wpi.edu}\\
              \email{sidner@wpi.edu}\\
%             \emph{Present address:} of F. Author  %  if needed
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}

Collaborative robots must be desinged to address the two challenging aspects of
sociability and working with humans on shared activities in the same
environement. The existing prominent collaboration theories provide frameworks
for the collaboration structure, but leave the underlying processes used for
generating and maintaining that structure unexplained. Humans are social and
emotional beings. We posit that emotions can explain the underlying processes in
a collaboration and provide mechanisms to model these in robots. In this paper,
four collaboration examples are provided to highlight the roles of emotion in
human-robot collaboration. The \textit{Affective Motivational Collaboration
Theory} is provided by us to explain, in detail, the underlying processes and
provide an approach for implementing them in robots. 

\keywords{Human-Robot Collaboration \and Emotional-Awareness \and Affective
Motivational Collaboration Theory}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

One aspect of the sociability of robots is their ability to collaborate with
humans in the same environment. Collaboration is a coordinated activity in which
the participants work jointly to satisfy a shared goal
\cite{grosz:plans-discourse}. There are many challenges in achieving a
successful collaboration between robots and humans in the same environment.
Therefore, It is crucial to understand what makes a collaboration not only
successful, but also effective. The existing computational collaboration
theories explain some of the important underlying concepts of collaboration such
as the precense of a reason for collaborators' committment, and the necessity
of communicating mental states between collaborators to maintain progress over
the course of a collaboration. The existing prominent collaboration theories are
mostly based on plans and joint intentions \cite{cohen:teamwork}
\cite{grosz:plans-discourse} \cite{Litman:discourse-commonsense}, and they are
derived from Bratman's BDI architecture \cite{bratman:intentions-plans}. The two
theories, Joint Intentions \cite{cohen:teamwork} and SharedPlans
\cite{grosz:planning-acting,grosz:collaboration,grosz:plans-discourse}, have
been extensively used to examine and describe teamwork and collaboration in
robots and virtual agents. However, these theories only explain the structure of
a collaboration. For instance, in SharedPlans theory collaborators build a
shared plan in which they have a collection of beliefs and intentions about the
actions in the plan. Also, collaborators communicate their beliefs and
intentions by making utterances about what actions they can contribute to the
shared plan. This communication leads to the construction of a shared plan, and
ultimately termination of the collaboration. On the other hand, in Joint
Intentions theory, the notion of joint intention is viewed as a persistent
commitment of the team members to a shared goal. In this theory, once an agent
entered into a joint commitment with other agents, it should communicate its
private beliefs with other team members.

We believe, although the existing collaboration theories exlain the necessity of
the important elements within a collaboration structure, there is still a lack
of a theory explaining the underlying processes required to dynamically create,
use, and maintain the elements of this structure. For instance, a general
mechanism has yet to be developed that allows an agent to effectively interact
with a human in a collaboration environment in case of a shared task failure.
Therefore, a theory comprising a process view of the collaboration requires to
embrace some elements of which a) they are inherently involved in social
interactions, and b) they essentially constitute processes capable of modifying
the content of social interaction. We believe emotions inherit these two
properties, and social functions of emotions can shed light on some aspects of
underlying processes in collaboration.

Humans are emotional and social being, therefore, emotions can also be social
and they are involved in developing different social contexts including
collaboration. Although emotions undeniably have personal aspects, they are
usually experienced in a social context and acquire their significance in
relation to this context \cite{parkinson:emotion-social-interaction}. For
instance, humans feel the emotions of those around them. They have emotions
about actions of those people around them. They have emotions about the things
that happen to other people. Also, humans' concern about their relationship with
others elicits emotion in them. The groups to which they belong can also elicit
their emotions. They can also feel emotion about the success and failure of
themselves, their own group, or of the others. Moreover, individuals sharing
emotions might possess a shared understanding of their environment, and socially
shared and regulated emotions can provide social meanings to the events
happening in that environment \cite{wisecup:sociology-emotions}. There is also a
communicative aspect of emotions. For instance, human behaviors around others
are performative which is often intended to convey information to others
\cite{goffman:self-presentation}. Or, when human's actions are visible in the
social context, they behave differently in the presence of the others
\cite{zajonc:social-facilitation}. Emotions are also invloved in one's verbal
behaviors. For instance, an utterance can include content and relational
meaning. An emotion might seem to be elicited by the content of the utterance,
but in fact it is an individual's response to the relational meaning
\cite{planalp:communicating-emotion}. The interpretation of these relational
meanings are handled by the appraisal of the events. Appraisal processes also
give us a way of viewing emotion as social \cite{hooft:sharing-emotions}.
Meaning is created by an individual's social relationships and experiences in
the social world, and individuals communicate these meanings through utterances.
Consequently, the meaning of these utterances and the emotional communication
change the dynamic of social interactions. A successful and effective emotional
communication necessitates ongoing reciprocal adjustments between interactants
that can happen by interpreting each other's behaviors
\cite{parkinson:emotion-social-interaction}. This adjustment procedure
fundamentally requires a baseline and an assessment process. While the
components of the collaboration strcuture, e.g., shared shared, provide the
baseline, emotions and underlying processes can provide the assessment
procedures.

We believe since collaboration is a special type of a social context, social
functions of emotions are required for an agent to perform in such an
environment. In this paper, we introduce two pairs of hypothetical examples.
Each pair provides an emotional-awareness and emotional-ignorant robot based on
our collaboration scenario. We also briefly discuss the \textit{Affective
Motivational Collaboration Theory} in which we introduce underlying mechanisms
each of which include one or multiple processes that we believe they are
required to be involved to produce collaborative behaviors. The
emotional-awareness examples show how these mechanisms are involved to
successfully finish a collaboration considering a) agreeing on a shared goal
with a robot (see Sections \ref{sec:exp1} and \ref{sec:wt-exp1}), and b)
delegating a new task to the robot (see Sections \ref{sec:exp3} and
\ref{sec:wt-exp3}). The emotional-ignorant examples are the same, except that
they ignore the human's verbally or nonverbally expressed emotions. The example
in Section \ref{sec:exp2} (see also Section \ref{sec:wt-exp2}) shows agreeing on
a shared goal can fail if the robot ignores the human's emotion. Similarly, the
example in Section \ref{sec:exp4} (see also Section \ref{sec:wt-exp4}) shows how
the delegation of a new task can fail if the robot ignores human's emotion. The
same four example in Sections \ref{sec:exp1} to \ref{sec:exp4} are discussed
in more details in our walkthrough examples in Section \ref{sec:wtce}. In this
section we use the same mechanisms introduced in Section
\ref{sec:computational-framework} to show how each meachanism is invloved to
produce the utterances within our examples in Section
\ref{sec:example-scenario}.

\section{Example Scenario}
\label{sec:example-scenario}
%Text with citations \cite{RefB} and \cite{RefJ}.

\subsection{The Backstory}

The scenario transpires in a facilty building collaborative robots to work with
astronauts. The mission is to finish installing the required solar panels to
provide energy for the operation of science labs on the moon. Most of these
panels have already been installed. However, the operation is now faced with low
batteries which forces the operation team to be cautious about consuming energy.
The astronaut is inspecting the working conditions in the field and planning the
installation of the remaining panels in collaboration with the robot.

\subsection{Astronaut-Robot Interaction}

The robot and the astronaut will collaborate with each other to achieve their
shared goal, which is to install two solar panels. They will face various
difficulties, ranging from the task being unpleasant and challenging to
conflicts of their private and/or shared goals occurring because of a blocked or
a protracted sub-task. The robot and the astronaut will go through a series of
assessment processes to figure out a) how did the current blocking happen? b)
why is the current task is blocked? and c) what is the next action they are
going to take? The robot uses its cognitive abilities and its communication
skills to overcome these problems and to motivate the astronaut to propose
alternative tasks. The following is part of an interaction between the astronaut
and the robot during their collaboration on installing solar panels.

\subsection{Agreeing on Shared Goal (Emotion-Awareness)}
\label{sec:exp1}

This and the next hypothetical examples show that agreeing on a shared goal
requires the Robot to be aware of its collaborator's emotions (here,
frustration). In this example, the Astronaut's first turn (A1), shows her
verbally conveying her frustration with respect to the disfunctioning
measurement tool that is used for checking the quality of the installed panel.
In return, the Robot's first turn (A2), as the crucial part of this interaction,
shows the Robot perceiving the Astronaut's frustration and acknowledging that
verbally.\footnote{The underlined section of the Robot's utterances (in turn A2)
shows the influence of using emotion-driven processes which leads to
acknowledgement of the Astronaut's emotion. See the absence of these utterances
as the consequence of ignoring the Astronaut's emotions in the same turn in the
next example (Section \ref{sec:exp2}).} Later on, in Section \ref{sec:wt-exp1},
we are going to show how the computational mechanisms, discussed in Section
\ref{sec:AMCT}, are involved in this process. In other words, we are going to
discuss how the emotion-driven goal-directed mechanisms can work together and
lead the Robot's behavior to acknowledge the perceived emotion of the Astronaut
properly, in order to avoid termination of the collaboration. Continuing in turn
A3, the Astronaut's utterance shows the change of the underlying belief from
termination of the collaboration to a belief in the possibility of seeking
instrumental support by asking the Robot whether it is possible to fix the
measurement tool. Notice that the proper acknowledgement of the Astronaut's
emotion helps to change her emotion from frustration to neutral. Now that the
Astronaut does not express a negative emotion (i.e., frustration), and she is
asking for instrumental support, the Robot can provide the alternative task as a
potential solution (A4). Here is another advantage of the emotion-awareness in
this hypothetical example. Although, the Robot, according to the shared plan
(see Sections \ref{sec:collaboration-theories} and \ref{sec:AMCT}), could
provide the same alternative task as a solution to the Astronaut immediately, it
procrastinated instead, providing the potential solution based on the
Astronaut's negative emotional state, i.e., frustration. Finally, since agreeing
on a shared goal is a collaborative negotiation process, emotion-awareness plays
a crucial role in providing a fair offer to the collaborator during negotiation.
As a result, the Astronaut's response in the last turn (A5) shows the acceptance
of the Robot's potential solution to continue collaboration and agreement on the
shared goal while she is content. In the next example we are going to show what
happens to the same hypothetical example when the Robot ignores the Astronaut's
emotion and tries to save the collaboration process from failure.\\

\begin{spacing}{1.3}
\small{ 
\begin{description}
  \item \textit{\textbf{A1. Astronaut:}} Oh no! Finishing the quality check of
  our installation with this measurement problem is so frustrating. I think we
  should stop now!\\

  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{A2. Robot:}}
  \underline{I see. This is frustrating.} But, I can help you with the
  measurement tool and we can finish the task as originally planned.
  \end{varwidth}}\\
  
  \item \textit{\textbf{A3. Astronaut:}} Can you fix the measurement tool?\\

  \item \textit{\textbf{A4. Robot:}} The next task is fixing the panel and it
  needs you to prepare and attach the welding rod to your welding tool. To save
  our time, I will fetch another measurement tool while you are preparing your
  welding tool.\\

  \item \textit{\textbf{A5. Astronaut:}} That would be great!
  
\end{description}
}
\end{spacing}

\subsection{Agreeing on Shared Goal (Emotion-Ignorance)}
\label{sec:exp2}

This example shows the same process of agreeing on a shared goal as previous
one except that it diverges from reaching to an agreement, despite the fact
that it begins with the same utterance (B1) as it appears in previous example
(A1). As mentioned earlier in Section \ref{sec:exp1}, the emotion-awareness is
beneficial in collaboration by channeling the collaboration process towards the
shared goal in the right direction. Without the emotion-awareness a
collaborative Robot will try to maintain the status of the shared goal and
prevent it from failure without considering its collaborator's negative emotion
which can be a direct result of a type of task failure during collaboration.
First, the emotion-ignorant Robot does not acknowledge the Astronaut's
frustration (i.e., B2 in compare to A2 in Section \ref{sec:exp1}), since it does
not perceive that emotion. Then, while negotiating the shared goal the Robot
fails to offer a potential solution with respect to the Astronaut's emotional
state which is the reflection of Astronaut's overall mental state. As a result,
it causes the failure of the negotiation procedure during collaboration.

In this example, the Robot is not capable of perceiving the Astronaut's emotion,
thus it does not apply the Astronaut's emotion (i.e., frustration) as an
influential factor in its computational mechanisms (see Section \ref{sec:AMCT}).
Hence, in the Robot's first response to the Astronaut's utterances (B2), first
it does not acknowledge the Astronaut' emotion, and second, it immidiately
conveys two available alternative actions according to the existing shared plan
(see Section \ref{sec:collaboration-theories}) and asks the Astronaut to select
between these two actions.\footnote{Notice that the Robot's acknowledgement of
the Astronaut's emotion is missing in B2 because of ignoring the Astronaut's
emotion -- see the Robot's same turn (A2) in Section \ref{sec:exp1}. Also,
notice (for the same reason) that the underlined uttrance in B2, reveals that
the Robot requires the Astronaut's confirmation for the next step.} As it
appears in the Astronaut's response, the Robot's immidiate proposal does not
make any progress in collaboration. As a result, the Astronaut just repeats
herself about the task status (B3) while still expresses frustration. The
Astronaut's response does not change the Robot's mental state and this causes
the Robot to try to repeat its own question while still is missing the
Astronaut's frustration (B4). The Robot's utterance creates an ambiguious
assumption for the Astronaut on whether the Robot can fix the broken measurement
tool for her. This ambiguity makes the Astronaut even more frustrated and causes
her to ask a question to remove the ambiguity of the Robot's proposal (B5). In
return, the Robot not only misses the Astronaut's intensified frustration, but
it also nullifies the Astronaut's assumption about fixing the disfunctional
measurement tool and proposes the potentioal solution of replacing the broken
tool and seeing whether the Astronaut agrees on that (B6). In B7, the Astronaut
modifies its assumption and announces the shortage of time as justification of
expressing her anger. At this point, the Robot's response becomes more crucial
since its wrong way of interaction and emotion-ignorance behavior shift the
Astronaut's emotional and mental states into a noncollaborative status.
Consequently, again, the Robot wants to revive the collaboration process,
therefore it provides more information about the repercussions of terminating
the collaboration process and wants to see whether the Astronaut can pursue
another task (B8).\footnote{Notice the underlined section of the Robot's turn
B8 indicates its dissociated reasoning about the problem from the Astronaut's
mental state.} Finally, the incorrect interaction of the Robot caused by its
emotion-ignorance behaviors which was developed since the Robot's very first
response and has continued till the end, leads to an unsuccessful termination of
their collaboration (B9).\\

\begin{spacing}{1.2}
\small{
\begin{description}
  \item \textit{\textbf{B1. Astronaut:}} Oh no! Finishing the quality check of
  our installation with this measurement problem is so frustrating. I think we
  should stop now!\\

  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{B2. Robot:}} I can
  help you with the measurement tool, or we can terminate this task.
  \underline{What do you want me to do?}\end{varwidth}}\\
  
  \item \textit{\textbf{B3. Astronaut:}} As I said the measurement tool does not
  work properly. We can not continue!\\

  \item \textit{\textbf{B4. Robot:}} Okay. Do you want me to fix this problem
  or terminate the task?\\

  \item \textit{\textbf{B5. Astronaut:}} Can you fix my measurement tool?\\
  
  \item \textit{\textbf{B6. Robot:}} I cannot fix your measurement tool, but I
  can fetch another one for you if you want?\\
  
  \item \textit{\textbf{B7. Astronaut:}} No, I don't want another measurement tool!
  We don't have time for that!\\
  
  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{B8. Robot:}} Okay.
  You want me to terminate this task. Terminating this task can influence the
  quality of installing this solar panel which can cause the mission to fail.
  Or, \underline{do you want us to work on another task?} This can help us to
  install the panel using your welding tool, but I do not know whether the
  quality of our installation will be acceptable.\end{varwidth}}\\
  
  \item \textit{\textbf{B9. Astronaut:}} I told you we have this problem and we
  should terminate the mission! We cannot continue without the measurement
  tool!
\end{description}
}
\end{spacing}

\subsection{Delegation of a Task (Emotion-Awareness)}
\label{sec:exp3}

This and the next hypothetical examples as a completely different collaborative
behaviors show how delegation of a task critically depends on understanding how
worried the other collaborator is and the necessity of having sufficient time,
which play together. This example shows when the Robot is aware of the
Astronaut's worriedness, it can use its own motivation mechanism drived by
emotions to come up with a way to alleviate that. Its methods are to exactly
postpone any questions until such time as they are critical (effects are
appeared in C2, C4). At the beginning of this example (C1), Astronaut is worried
because of the lack of time in achieving the shared goal (finishing to install
solar panels) for the overall task. She proposes the Robot to begin installing
the second panel, since the first one still has some problems. The Robot in its
first turn (C2), perceives the Astronaut's emotion (i.e., worriedness) and using
the same cognitive mechanisms (see Section \ref{sec:AMCT}) acknowledges the
Astronaut's emotion just as it did in first example in Section
\ref{sec:exp1}.\footnote{The underlined utterance in the Robot's turn C2, shows
the Robot's awareness of the Astronaut's emotion.} Also, Robot does not ask the
Astronaut (because of perceiving worriedness) if it is okay to leave the current
task which was helping the Astronaut to install the first panel. As mentioned
earlier, we are going to show step-by-step (see Sections \ref{sec:wt-exp1} and
\ref{sec:wt-exp3}) how different emotion-driven goal-directed mechanisms get
invloved in each turn of the collaboration process to appropriately respond to
the Astronaut based on the \textit{Affective Motivational Collaboration Theory}
(see Section \ref{sec:AMCT}). Continuing in this example, after acknowledging
the Astronaut's emotion, the Robot infers that it needs to postpone asking
questions about the missing parts of the shared plan (see Section
\ref{sec:collaboration-theories}) since installing a panel is a collaborative
task and some of the primitive tasks need to be done by the Astronaut. Then, the
Astronaut perceives the Robot's response as a proper acceptance of the task
delegation and tries to communicate the status of her own, even though she is
still worried about finishing the overall task (C3). Now, the Robot perceives
the Astronaut's worriedness and without asking the actual detailed questions
about the delegated task, declares the possibility of asking some follow-up
questions whenever the Astronaut's answers are required to make progress in
executing some primitive tasks while installing the second panel (C4). Here, the
Robot not only prevents overwhelming the Astronaut with several questions
related to executing the next steps in the future, but its utterance implicitly
reveals its knowledge of tasks' requirements to the Astronaut. As a result, the
Robot's appraoch on acceptance of the delegated task mitigates the Astronaut's
negative emotion (i.e., worriedness) which makes her to respond possitively to
the Robot's proposal. The next example explains the same hypothetical task
delegation scenario except it shows the negative impact of missing the
Astronaut's emotion on task delegation process.\\

\begin{spacing}{1.2}
\small{
\begin{description}
  \item \textit{\textbf{C1. Astronaut:}} I still have some problems with
  attaching the first panel! We do not have enough time. You should begin to
  install the second panel.\\

  \item \fbox{\begin{varwidth}{0.96\textwidth}\textit{\textbf{C2. Robot:}} Okay.
  \underline{Don't worry.} I can handle that.

  [\textit{Robot perceives the Astronaut's request as on “open planning” (as
  oppose to specific executive) task delegation.}]\end{varwidth}}\\
  
  \item \textit{\textbf{C3. Astronaut:}} I will try to fix it asap.\\

  \item \textit{\textbf{C4. Robot:}} I might need to ask some questions while I
  am installing the second panel.\\

  \item \textit{\textbf{C5. Astronaut:}} That's fine. Just let me know.
  
\end{description}
}
\end{spacing}

\subsection{Delegation of a Task (Emotion-Ignorance)}
\label{sec:exp4}

This last hypothetical example is also about task delegation (similar to the
example in Section \ref{sec:exp3}) and it shows how ignoring the collaborator's
emotions in task delegation procedure can negatively impact the progress of a
collaboration. In this example, the emotion-ignorant Robot is doing planning in
its most efficient manner (efficient because time is short); asking a lot of
questions (see utterances in D2, D4, D6, D8, D10) so that it can work out the
plan. But asking questions exacerbates the Astronaut's worry which leads to an
unsuccessful collaboration due to the lack of time. 

As it is shown, the very first utterance of the Astronaut (D1) is the same as
the first utterance in previous example (C1). The Astronaut is worried and
expresses her worry. However, the Robot is not capable of perceiving and
consequently acknowledging the Astronaut's emotion. As a result, the Robot
responds to the Astronaut while the Astronaut's proposed task changes the
Robot's focus of attention to a new task, and now, the Robot tries to determine
a proper solution for an action selection problem. The reason that the Robot
perceives the Astronaut's proposal as an action selection problem is primarily
caused by the shift in the Robot's focus of attention from an unfinished ongoing
task (unsatisfied postconditions) to a new partially known nonprimitive task
(i.e., installing the second panel). Therefore, the Robot immediately tries to
confirm leaving the current unfinished task (D2). Notice the absence of
acknowledging the Astronaut's emotion by the Robot in this turn (compare C2 in
Section \ref{sec:exp3} and D2 here). This vacancy of the emotional awareness is
the beginning of the failure of the task delegaton process. As we can see, the
Robot's response does not mitigate the Astronaut's worriedness about the future
of the collaboration.\footnote{The underlined section in D2 shows the Robot's
need for confirmation of leaving an unfinished task.} Next, the Astronaut tries
to help the Robot to select the proper action by responsing possitively about
the Robot leaving the current task (D3). Now, the Robot shifts its focus of
attention to the new task and uses the Collaboration mechanism (see Section
\ref{sec:AMCT}) to obtain required information such as task dependencies,
existing preconditions and required resources as well as the existance of a
plan. Subsequently, the absence of the required information and dependencies to
the Astronaut's actions causes the Robot to ask a question about whether the
Astronaut will be able to help in some parts of the task (D4). Although, the
Robot's interactive behavior is crucial in many collaborative contexts, here it
implies that the Astronaut's worriedness caused by the lack of time is ignored
by the Robot. This ignorance of the Astronaut's emotional state influences her
mental state and consequently her behavior. Thus, she succinctly responds to
the Robot's question while she is still worried about finishing of installing
the first panel (D5). The Robot asks another question about the required inputs
for the task which are dependent to the Astronaut without considering her
worriedness (D6). At this point, since the Astronaut believes that the Robot's
questions are unneccessary, she becomes frustrated and impatiently answers the
Robot's question (D7). However, once again, not only the Robot misses the
Astronaut's emotion, but it also wants to prevent failure of a task in the
future (D8).\footnote{Notice that the undelined section in D8 is the result of
the Robot's inference about the possibility of a future problem.} Then, the
Astronaut does not recall the event referenced by the Robot and since she is
frustrated, she does not even try to remove the ambiguity of the existing issue
(D9). Therefore, once again, the Robot misses the Astronaut's frustration and
tries to see whether the Astronaut wants the Robot to clarify the issue for her
by providing her some examples (D10).\footnote{The underlined utterance in D10
indicates another situation in which the Robot misses the Astronaut's emotion.}
At last, the Astronaut terminates the collaboration task because of the lack of
time (D11).\\

\begin{spacing}{1.2}
\small{
\begin{description}
  \item \textit{\textbf{D1. Astronaut:}} I still have some problems with
  attaching the first panel! We do not have enough time. You should begin to
  install the second panel.\\

  \item \fbox{\begin{varwidth}{0.96\textwidth} \textit{\textbf{D2. Robot:}}
  Okay. I can handle that. \underline{Is it okay if I leave the current task
  though?}\end{varwidth}}\\
  
  \item \textit{\textbf{D3. Astronaut:}} Yes, that's fine. I can finish it by my
  own.\\

  \item \textit{\textbf{D4. Robot:}} Okay. But according to the plan, I need
  your help when we need to weld the panel. Will you be able to help me?\\

  \item \textit{\textbf{D5. Astronaut:}} Yes, I will.\\
  
  \item \textit{\textbf{D6. Robot:}} We also need your welding tool and supplies
  as well as the measuring tool to install the second panel.\\
  
  \item \textit{\textbf{D7. Astronaut:}} Just let me know whenever you need
  something!\\
  
  \item \fbox{\begin{varwidth}{0.96\textwidth} \textit{\textbf{D8. Robot:}}
  Thanks. I will. \underline{How about the conflict that might occur while I am
  trying} \underline{to fix the second panel?} You are going to need my help at
  the same time.\end{varwidth}}\\
  
  \item \textit{\textbf{D9. Astronaut:}} Robot, I really don't understand what
  you are talking about!\\
  
  \item \fbox{\begin{varwidth}{0.96\textwidth} \textit{\textbf{D10. Robot:}}
  \underline{Do you want me to provide some examples?} \end{varwidth}}\\

  \item \textit{\textbf{D11. Astronaut:}} We don't have time for this anymore!
  
\end{description}
}
\end{spacing}

\section{Computational Framework}
\label{sec:computational-framework}

As we mentioned in Section \ref{sec:example-scenario}, all provided examples in
Sections \ref{sec:exp1} to \ref{sec:exp4} will be discussed in more details in
Sections \ref{sec:wt-exp1} to \ref{sec:wt-exp4} using our computational theory
called \textit{Affective Motivational Collaboration Theory}. There are several
mechanisms (see Fig. \ref{fig:theory}) involved in our computational framework
applying different concepts of the \textit{Affective Motivational Collaboration
Theory} to provide an emotion-regulated collaborative behavior for a robot. In
this section, we are going to breifly describe all these mechanisms. However, we
will not elaborate the underlying processes involved in each mechanism due to
the scope of this paper. We also explain different types of associated mental
states in our computational framework based on the \textit{Affective
Motivational Collaboration Theory}.

\subsection{Affective Motivational Collaboration Theory}
\label{sec:AMCT}

\textit{Affective Motivational Collaboration Theory} is about the interpretation
and prediction of the observable behaviors in a dyadic collaborative
interaction. There are several theories which describe the underlying structure
of a collaboration based on mental states of the collaborators (see Section
\ref{sec:collaboration-theories}). The collaboration structure of
\textit{Affective Motivational Collaboration Theory} is based on the SharedPlans
theory \cite{grosz:planning-acting,grosz:collaboration,grosz:plans-discourse}.
\textit{Affective Motivational Collaboration Theory} focuses on the processes
that generate, maintain and update this structure based on mental state. The
collaboration structure is important because social robots ultimately need to
co-exist with humans, and therefore need to consider humans mental state as
well as their own internal states and operational goals. The processes involved
in collaboration are important because they explain how the collaboration
structure is formed and dynamically evolved based on the collaborators'
interaction.

The \textit{Affective Motivational Collaboration Theory} focuses on the
processes regulated by emotional states. It aims to explain both rapid emotional
reactions to events as well as slower, more deliberative responses. These
observable behaviors represent the outcome of reactive and deliberative
processes related to the interpretation of the Robot's relationship to the
collaborative environment. These reactive and deliberative processes are
triggered by two types of events: \textit{external} events, such as the human's
\textit{utterances} and \textit{primitive actions}, and \textit{internal}
events, comprising changes in the Robot's mental state, such as belief formation
and emotional changes. \textit{Affective Motivational Collaboration Theory}
explains how emotions regulate the underlying processes in the occurrence of
these events during collaboration.

Emotion-regulated processes operate based on the Robot's mental state including
the anticipated mental state of the human, generated according to the Robot's
model of the human. These mental states include beliefs, intentions, goals,
motives and emotion instances. Each of these mental states possesses multiple
attributes impacting the relation between cognition and behavior or perception.

\begin{figure}[h!]
  \includegraphics[scale=0.78]{figure/theory-general-croped.pdf}
  \caption{Computational framework based on \textit{Affective Motivational
  Collaboration Theory} (primary influences between mechanisms).}
  \label{fig:theory}
\end{figure}

\subsection{Underlying Mechanisms}
\label{sec:mechanisms}

The \textit{Affective Motivational Collaboration Model} consists of seven
mechanisms (see Fig.~\ref{fig:theory}) most of which directly store and
fetch the data in the Mental States.

\subsubsection{Collaboration Mechanism}
\label{sec:collaboration-mech}

The \textit{Collaboration} mechanism (see Fig.~\ref{fig:theory}) will construct
a hierarchy of tasks and also manage and maintain the constraints and other
required details of the collaboration specified by the plan. These constraints
on task states and on the ordering of tasks include the inputs and outputs of
individual tasks, the preconditions specifying whether it is appropriate to
perform a task, and the postconditions specifying whether a just-completed task
was successful (which can be used as an indication of an impasse or failure).
The Collaboration mechanism provides processes to update and monitor the shared
plan. It also keeps track of the focus of attention, which determines the
salient objects, properties and relations at each point of the collaboration.
These processes depend on the operation of other mechanisms. For instance,
Appraisal mechanism is required to evaluate the current mental state with
respect to the current status of the collaboration. Also, the Appraisal and
Motivation mechanisms provide interpretation of task failure and the formation
of new mental state (e.g.\,intentions) respectively.

\subsubsection{Appraisal \& Coping Mechanisms}
\label{sec:appraisal-coping-mech}

Appraisal is a subjective evaluation mechanism based on individual processes
each of which computes the value of the appraisal variables. The Appraisal
mechanism is responsible for evaluating changes in the Robot's mental state, the
anticipated mental state of the human, and the state of the collaboration
environment. Collaboration needs the evaluative function of the Appraisal
mechanism for various reasons. The course of a collaboration is based on a full
or a partial plan which needs to be updated as time passes and collaborators
achieve, fail at or abandon a task assigned to them. The failure of a task
should not destroy the entire collaboration. Appraising the environment and the
current event helps the Robot to update the collaboration plan in response to
changes in the environment and avoid further critical failures during
collaboration. Appraisal also helps the Robot to have a better understanding of
the human's actions by making inferences based on appraisal variables.
Furthermore, in order to collaborate successfully, a collaborator cannot simply
use the plan and reach to the shared goal; there should be an adaptation
mechanism not only for updating the plan but also the underlying mental state.
The output of Appraisal can directly and indirectly impact other mechanisms. For
instance, the Motivation mechanism uses this data to generate, compare and
monitor motives based on the current internal appraisal of the Robot as well as
the appraisal of the environment. The Coping mechanism is responsible for
adopting the appropriate behavior with respect to interpretion of the ongoing
internal and external changes. The Coping mechanism provides the Robot with
different coping strategies associated with changes in the Robot's mental state
with respect to the state of the collaboration. In other words, the Coping
mechanism produces cognitive responses based on the appraisal patterns.

\subsubsection{Motivation Mechanism}
\label{sec:motivation-mech}

The \textit{Motivation} mechanism (see Fig.~\ref{fig:theory}) operates whenever
the Robot a) requires a new motive to overcome an internal impasse in an ongoing
task, or b) wants to provide an external motive to the human when the human
faces a problem in a task. In both cases, the Motivation mechanism uses the
Appraisal mechanism to compute attributes of the competing motives. The purpose
of Motivation mechanism is to generate new emotion-driven goal-directed motives
considered as ``potential'' intentions. These motives are generated based on what
the Robot believes about the environment including Robot and the other
collaborator and the corresponding appraisals. The Robot uses these motives to
reach to a private or shared goal according to new conditions caused by changes
in the environment. The Motivation mechanism consists of an arrangement of three
distinct processes. First, several motives can be generated with respect to the
current mental state. Only one of these competing motives is most likely to
become a new intention. Therefore, a comparator decides which motive is more
likely to be consistent with the current state based on the values of the motive
attributes (e.g., motive insistence and motive urgency). Finally, the new motive
will be used to form a new intention. As a result, the Robot can take an action
based on the new intention to sustain the collaboration progress. Furthermore,
the Motivation mechanism can serve the Theory of Mind mechanism by helping the
Robot to infer the motive behind the human's current action.

\subsubsection{Theory of Mind Mechanism}
\label{sec:tom-mech}

The \textit{Theory of Mind} mechanism (see Fig.~\ref{fig:theory}) is the
mechanism of inferring a model of the human's anticipated mental state. The
Robot uses the Theory of Mind mechanism to infer and attribute beliefs,
intentions, motives and goals to its collaborator based on the user model it
creates and maintains during collaboration. The Robot progressively updates this
model during the collaboration. The refinement of this model helps the Robot to
anticipate the human's mental state more accurately, which ultimately impacts
the quality of the collaboration and the achievement of the shared goal.
Furthermore, the Robot can make inferences about the motive (or intention) behind
the human's actions using the Motivation mechanism. This inference helps the
Robot to update its own beliefs about the human's mental state. In the reverse
appraisal process \cite{gratch:reverse-appraisal}, the Robot also applies the
Appraisal mechanism together with updated beliefs about the human's Mental
States to infer the human's current mental state based on the human's emotional
expression. Finally, the Collaboration mechanism provides the collaboration
structure, including status of the shared plan with respect to the shared goal
and the mutual beliefs to the Theory of Mind mechanism. Consequently, any change
to the Robot's model of the human will update the Robot's mental state.

\subsubsection{Perception \& Action Mechanisms}
\label{sec:tom-mech}

The Perception mechanism is responsible for producing the sensory information
used by other mechanisms in our framework, and it is only a source of data to
the computational framework (see Fig. \ref{fig:theory}). Thus, the computational
framework starts with high-level semantic representation of events (including
utterances). The output of the Perception mechanism will be given to the
Collaboration, Theory of Mind and Appraisal mechanisms and provides a unified
perception representation across all of these mechanisms.

The Action mechanism functions whenever the Robot needs to show a proper
behavior according to the result of the internal processes of the collaboration
procedure, and it is only a sink of data in our computational framework (see
Fig. \ref{fig:theory}). The input to the Action mechanism is provided by the
Coping mechanism. This data will cause the Action mechanism to execute an
appropriate behavior of the Robot. This data has the same level of abstraction
as the output of the Perception mechanism, i.e., it includes Robot's utterances,
primitive actions and emotional expressions.

\subsection{Mental States \& Emotion Instances}

The Mental States shown in Fig.\,\ref{fig:theory} comprise the knowledge base
required for all the mechanisms in the overall framework.

\subsubsection{Beliefs}
\label{sec:beliefs}

\textit{Beliefs} are a crucial part of the Mental States. We have two different
perspectives on categorization of beliefs. In one perspective, we categorize
beliefs based on whether they are shared or not between the collaborators. The
SharedPlans \cite{grosz:plans-discourse} theory is the foundation of this
categorization in which for any given proposition the Robot may have: a) private
beliefs (the Robot believes the human does not know these), b) the inferred
beliefs of the human (the Robot believes the human collaborator has these
beliefs), and c) mutual beliefs (the Robot believes both the Robot and the human
have these same beliefs and both of them believe that). From another
perspective, we categorize beliefs based on who or what they are about. In this
categorization, beliefs can be about the Robot, the human, or they can be about
the environment. Beliefs about the environment can be about internal events,
such as outcomes of a new appraisal or a new motivate, or external events such
as the human's offer, question or request, and general beliefs about the
environment in which the Robot is situated. Beliefs can be created and updated
by different processes. They also affect how these processes function as time
passes.

\subsubsection{Intentions}
\label{sec:intentions}

\textit{Intentions} are mental constructs directed at future actions. They play
an essential role in: a) taking actions according to the collaboration plan, b)
coordination of actions with human collaborator, c) formation of beliefs about
Robot and anticipated beliefs about the human, and d) behavior selection in the
Coping mechanism. First, taking actions means that the Robot will intend to take
an action for primitive tasks that have gained the focus of attention, possess
active motives, have satisfied preconditions for which required temporal
predecessors have been successfully achieved. Second, intentions are involved
in action coordinations in which the human's behavior guides the Robot to infer
an anticipated behavior of the human. Third, intentions play a role in belief
formation mainly as a result of the permanence and commitment inherent to
intentions in subsequent processes, e.g., appraisal of the human's reaction to
the current action and self regulation. And lastly, intentions are involved in
selecting intention-related strategies, e.g., planning, seeking instrumental
support and procrastination, which these strategies are an essential category of
the strategies in the Coping mechanism \cite{marsella:ema-process-model}.
Intentions possess a set of attributes, e.g. \textit{Involvement, Certainty,
Ambivalence} which moderate the consistency between intention and behavior. The
issue of consistency between the intentions (in collaboration) and the behaviors
(as a result of the Coping mechanism in the appraisal cycle) is important
because neither of these two mechanisms alone provides solution for this
concern.

\subsubsection{Motives}
\label{sec:motives}

\textit{Motives} are emotion-driven goal-directed mental constructs which can
initiate, direct and maintain goal-directed behaviors. They are created by the
emotion-regulated Motivation mechanism. Motives can cause the formation of a new
intention for the Robot according to: a) its own emotional states (how the Robot
feels about something), b) its own private goal (how an action helps the Robot
to make progress), c) the collaboration goal (how an action helps to achieve the
shared goal), and d) human's anticipated beliefs (how an action helps the
human). Motives also possess a set of attributes, e.g., \textit{Insistence} or
\textit{Failure Disruptiveness}. These attributes are involved in comparison of
newly generated motives based on the current state of the collaboration.
Ultimately, the Robot forms or updates an intention about the winning motive in
the Mental States.

\subsubsection{Goals}
\label{sec:goals}

\textit{Goals} help the Robot to create and update the structure of the
collaboration plan. Goals direct the formation of intentions to take appropriate
corresponding actions during collaboration. Goals also drive the Motivation
mechanism to generate required motive(s) in uncertain or ambiguous situations,
e.g., to minimize the risk of impasse or to reprioritize goals. Goals have
three attributes. The \textit{Specificity} of goals has two functions for the
Robot. First, it defines the performance standard for evaluating the progress
and quality of the collaboration. Second, it serves the Robot to infer the
winner of competing motives. The \textit{Proximity} of goals distinguishes goals
according to how ``far'' they are from the ongoing task. Proximal (or
short-term) goals are achievable more quickly, and result in higher motivation
and better self-regulation than more temporally distant (or long-term) goals.
Goals can influence the \textit{Strength} of beliefs, which is an important
attribute for regulating the elicitation of social emotions. The
\textit{Difficulty} of goals impacts collaborative events and decisions in the
appraisal, reverse appraisal, motive generation and intention formation
processes. For instance, overly easy goals do not motivate; neither are humans
motivated to attempt what they believe are impossible goals.

\subsubsection{Emotions}

\textit{Emotions} in Mental States are emotion instances that are elicited by
the Appraisal mechanism. These emotion instances include the Robot's own
emotions as well as the anticipated emotions of the human which are created with
the help of the processes in the Theory of Mind mechanism.

\section{Walk Through Computational Examples}
\label{sec:wtce}

In this section, we are going to discuss how the individual computational
mechanisms (see Section \ref{sec:mechanisms}) are involved in generating the
Robot's collaborative behaviors discussed in each example in Section
\ref{sec:example-scenario}. The following four walkthrough examples are in the
same order as the four examples in Section \ref{sec:example-scenario}. These
examples can demonstrate the applicability of the \textit{Affective Motivational
Collaboration Theory} in modelling and understanding of the emotion-regulated
underlying processes of a collaboration procedure. Notice that in emotional
ignorance examples (Sections \ref{sec:wt-exp2} and \ref{sec:wt-exp4}), we use
the same mechanisms as in the emotional awareness examples.

Also, the utterances in our examples in Section \ref{sec:example-scenario} are
repeated here inside of boxes to help better understanding of the associated
details in our walkthroughs. The walkthrough explanations between these boxes
use a simple and straightforward representation to show how the underlying
mechanisms (see Section \ref{sec:computational-framework}) are invloved to
generate collaborative responses for the Robot.

\subsection{Agreeing on Shared Goal (Emotion-Awareness)}
\label{sec:wt-exp1}

This Section provides a step-by-step walkthrough explanation of the same
example presented in Section \ref{sec:exp1}. In this example, the explanation
between Astronaut's utterance A1 and Robot's utterance A2 illustartes how the
Robot perceives and interprets events including Astronaut's utterances and
emotional expressions, and how the Robot takes an appropriate action whenever it
is required.\footnote{Since our walkthrough explanation of underlying processes
is based on collaborators' utterances, we use \textbf{verbal} expression of
emotions within the utterances to emphasize their existance in certain parts of
the collaboration. However, although the nonverbal emotional expressions (e.g.,
facial expressions) can provide the same impact during collaboration, the
automatic recognition of them is out of our research context.}\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}
\textit{\textbf{A1. Astronaut:}} Oh no! Finishing the quality check of our
installation with this measurement problem is so frustrating. I think we should
stop now!\end{varwidth}}\\ \\

%\fontsize{9pt}{10pt}\selectfont
\noindent \textbf{(Perception)} Robot perceives Astronaut's utterances and
emotion.\\
  
First, the Robot perceives the Astronaut's utterances as well as her emotion in
the first turn, i.e., A1. The perception mechanism (see Section \ref{sec:AMCT})
forms beliefs about the task under the Astronaut's focus of attention, and also
the Astronaut's emotion which she has expressed both verbally and nonverbally.
The beliefs formed about the task (i.e., installing the panel) include:

\begin{itemize}
  \item[$\bullet$] the Astronaut's proposal of \textit{stopping} the task,
  \item[$\bullet$] which is a \textit{future} event,
  \item[$\bullet$] and is \textit{caused by} the measurement tool problem.
\end{itemize}

\noindent Also, beliefs formed about the Astronaut's emotion (i.e., frustration)
include:

\begin{itemize}
  \item[$\bullet$] the existence of a \textit{negative valenced} emotion,
  \item[$\bullet$] which maps into a three-value vector of \textit{pleasure},
  \textit{arousal}, and \textit{dominance},
  \item[$\bullet$] and is verbally conveyed as \textit{frustration}.\\
\end{itemize}

\noindent\textbf{(Collaboration: \textit{Monitoring \& Focus Shifting})} Robot
uses the Collaboration mechanism (see Section \ref{sec:collaboration-mech}) to
form new beliefs about the collaboration status based on its perception. These
new beliefs are about:

\begin{itemize}
  \item[$\bullet$] the \textit{unsatisfied} precondition of the Astronaut's
  current \textit{task},
  \item[$\bullet$] the \textit{blocked} status of the Astronaut's current
  \textit{task},
  \item[$\bullet$] and consequently the \textit{blocked} status of the
  \textit{shared goal},
  \item[$\bullet$] which causes the change in the Robot's \textit{focus of
  attention} to the Astronaut's task.
\end{itemize}

\noindent \textbf{(Theory of Mind: \textit{Reverse Appraisal \& User
Modeling})} Robot uses reverse appraisal to understand the meaning of
Astronaut's frustration according to the collaborative task status (e.g.,
precondition and shared goal status). Robot updates Astronaut's user model
respectively.\\

The reverse appraisal process (see Section \ref{sec:appraisal-coping-mech})
forms beliefs about the anticipated appraisals of the Astronaut with respect to
the current task's status based on the Astronaut's utterances and emotion in A1,
and the output of the collaboration mechanism. Some of these anticipated
appraisal values indicate that the event is interpreted as \textit{relevant},
\textit{undesirable}, \textit{uncontrollable}, \textit{urgent}, and
\textit{unexpected} by the Astronaut. Furthermore, the user modeling process
updates the Astronaut's user model based on the output of the reverse appraisal
process and the collaboration mechanism; this user modeling process forms
beliefs that a) Astronaut has \textit{low autonomy}, and b) Astronaut is a
\textit{highly communicative} collaborator.\\

\noindent \textbf{(Appraisal)} Robot appraises Astronaut's utterances and
emotion.\\

The Appraisal mechanism simultaneously uses distinct processes to compute values
for each individual appraisal variable. The output of these processes provides a
vector of values describing the Robot's interpretation of the current event,
(A1). The outcome will be mapped to a particular emotion instance, i.e.,
worriedness, since the Astronaut is frustrated and the task is blocked which
makes the Robot to properly express an appropriate emotion as a response to the
Astronaut's emotional state. The robot can choose to express this emotion
verbally or nonverbally as part of its communication if it is required.\\

\noindent \textbf{(Motivation: \textit{Motive \& Intention Formation})}
Robot forms new motives according to the result of:

\begin{enumerate}[a)]
  \item appraisal with respect to the shared goal,
  \item reverse appraisal of the Astronaut's emotion,
  \item and the user model of the Astronaut. 
\end{enumerate}

Then, the motive comparator process compares current available motives and
sorts them based on their distance to the Astronaut's emotional state and
the achievement of the shared goal.\footnote{Here, the distance function is a
function of a) the Astronaut's emotional state as an admissible approximation
of her mental state, and b) how taking an action based on the corresponding
intention of a particular motive improves the possibility of the collaborators
reaching to the collaborators' mutually accepted shared goal.} The Robot,
ultimately, selects the most related motive and forms a new intention with
respect to the current status of collaboration. After this whole process,
Robot uses the coping mechanism to take an action based on the available
intention.\\

\noindent \textbf{(Coping)} Based on the current mental state, the Robot chooses
an emotion-focused coping strategy (see Section \ref{sec:appraisal-coping-mech})
and decides to acknowledge Astronaut's emotion, and provide an alternative
solution. Subsequently, the Robot responds to the Astronaut with appropriate
utterances appearing in A2.\\

\noindent \fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{A2. Robot:}}
I see. This is frustrating. But, I can help you with the measurement
tool and we can finish the task as originally planned. \end{varwidth}}\\

The Astronaut's new utterance (A3) provides the Robot with a new question about
whether the Robot can fix the measurement tool. The followings show how the
robot employs the same mechanisms described in Section
\ref{sec:computational-framework} which enables the Robot to negotiate with the
Astronaut to reach an agreement on the shared goal.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{A3.
Astronaut:}} Can you fix the measurement tool?\end{varwidth}}\\

Notice that the processes in the awareness function will be run again (similar
to what we had above) for the new utterance of the Astronaut (A3) before the
following processes are run. The new beliefs as the output of the awareness
function based on the Astronaut's new utterance are:

\begin{itemize}
  \item[$\bullet$] the precondition associated to the Astronaut's current
  \textit{task} is still \textit{unsatisfied},
  \item[$\bullet$] the status of the Astronaut's current \textit{task} is still
  \textit{blocked},
  \item[$\bullet$] and similarly the status of the \textit{shared goal} is
  still \textit{blocked},
  \item[$\bullet$] however, the Astronaut's question changes the Robot's
  \textit{focus of attention} to the measurement tool,
  \item[$\bullet$] also, the Astronaut's \textit{emotion} has changed to
  \textit{neutral}.
  \item[$\bullet$] but, her user model \textit{stays the same}, i.e.,
  having low-autonomy and being highly communicative.
\end{itemize}

%\begin{spacing}{1.15}
%\fontsize{9pt}{10pt}\selectfont
\noindent\textbf{(Collaboration)} The change in the focus of attention to the
measurement tool causes the Robot to check the availability of a recipe to fix
or replace the disfunctioning measurement tool. Robot finds a recipe to replace
the measurement tool.\\

\noindent\textbf{(Appraisal)} Robot appraises the possibility of
replacing the measurement tool with respect to: a) the status of the shared
goal, and b) the Astronaut's user model. Robot finds the replacement of the
measurement tool \textit{relevant}, \textit{desirable}, and
\textit{controllable}.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms new motives based on the next task according to the shared plan and
outcome of the appraisal of the possibility of replacing the measurement tool.
Robot forms the corresponding intentions with respect to the new motives.\\

Once again, the Robot uses the coping mechanim to take an action based on the
recent intention.\\

\noindent\textbf{(Coping)} Based on the current mental state, Robot
chooses to negotiate and offer an alternative action to the Astronaut. (A4)\\
%\end{spacing}

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{A4. Robot:}} The
next task is fixing the panel and it needs you to prepare and attach the welding
rod to your welding tool. To save our time, I will fetch another measurement
tool while you are preparing your welding tool.\end{varwidth}}\\

At this point, Astronaut is content with the way Robot outlined the shared goal
and responds respectively (A5). Robot uses the same awareness function as we
discussed above, therefore, Robot percieves and interprets the Astronauts
response as an agreement on their new shared goal.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{A5. Astronaut:}}
That would be great!\end{varwidth}}

\subsection{Agreeing on Shared Goal (Emotion-Ignorance)}
\label{sec:wt-exp2}

This walkthrough example begins with the same exact utterance as the previous
one, and it provides the corresponding details of the emotion-ignorant example
in Section \ref{sec:exp2}. To avoid redundant explanations, we refer to similar
procedures in previous walkthrough example.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B1. Astronaut:}}
Oh no! Finishing the quality check of our installation with this measurement
problem is so frustrating. I think we should stop now!\end{varwidth}}\\

\noindent\textbf{(Perception)} Robot only perceives Astronaut's utterances.
(B1)\\

Here, in the first step, Robot perceives the Astronaut's utterances and ignores
her expressed emotion in B1, i.e., frustration. Similarly to previous
example, the perception mechanism forms beliefs about the task under the
Astronaut's focus of attention. These beliefs include:

\begin{itemize}
  \item[$\bullet$] the Astronaut's proposal of \textit{stopping} the task,
  \item[$\bullet$] which is a \textit{future} event,
  \item[$\bullet$] and is \textit{caused by} the measurement tool problem.
\end{itemize}

\noindent Notice that beliefs about the Astronaut's emotion are formed
differently in compare to previous example, and they are based on neutral
emotion of the Astronaut, since the Robot ignores the Astronaut's actual
emotion instance, i.e., frustration.\footnote{In emotion-ignorant examples, we
assume Robot always perceives neutral emotion expressed by the Astronaut.}

\begin{itemize}
  \item[$\bullet$] the existence of a \textit{neutral valenced} emotion,
  \item[$\bullet$] which maps into a three-value vector of \textit{pleasure},
  \textit{arousal}, and \textit{dominance},
  \item[$\bullet$] and is verbally conveyed as \textit{neutral} emotion.\\
\end{itemize}

\noindent\textbf{(Collaboration)} Robot uses the collaboration mechanism to form
new beliefs about the collaboration status based on its perception. These new
beliefs are the same as the previous example:

\begin{itemize}
  \item[$\bullet$] the \textit{unsatisfied} precondition of the Astronaut's
  current \textit{task},
  \item[$\bullet$] the \textit{blocked} status of the Astronaut's current
  \textit{task},
  \item[$\bullet$] and consequently the \textit{blocked} status of the
  \textit{shared goal},
  \item[$\bullet$] which causes the change in the Robot's \textit{focus of
  attention} to the Astronaut's task.
\end{itemize}

\noindent\textbf{(Theory of Mind: \textit{Reverse Appraisal \& User Modeling})}
Robot uses reverse appraisal to understand the meaning of Astronaut's neutral
emotion according to the collaborative task status (e.g., precondition and
shared goal status). Robot updates Astronaut's user model respectively.\\

The reverse appraisal process, as we discussed in previous example, forms
beliefs about the anticipated appraisals of the Astronaut with respect to the
current task's status based on the Astronaut's utterances and emotion that the
Robot perceives, as well as the output of the collaboration mechanism. In this
example, since the Robot misses the actual expressed emotion by the Astronaut
(i.e., frustration) and perceives her with amiss neutral emotion, the
corresponding anticipated appraisal values indicate wrong interpretation of the
event. For instance, the output of the reverse appraisal mechanism could
indicate a \textit{relevant}, \textit{desirable}, \textit{controllable},
\textit{non-urgent}, and \textit{expected} interpretation of the current event
by the Astronaut. Furthermore, the user modeling process updates the Astronaut's
user model based on the output of the reverse appraisal process and the
collaboration mechanism; this user modeling process forms beliefs that a)
Astronaut has \textit{high autonomy}, and b) Astronaut is a
\textit{moderately communicative} collaborator.\\

\noindent\textbf{(Appraisal)} Robot appraises Astronaut's utterances.\\

The Appraisal mechanism operates similar to what we discussed in Section
\ref{sec:exp1}. The output of these processes provides a vector of values
describing the Robot's interpretation of the current event (B1). The outcome
will be also mapped to a particular emotion instance, but since the Robot misses
Astronaut's emotion, it maps the appraisals to a defferent emotion, i.e.,
hope, than the one elicited in previous example. Robot elicits hope because it
believes the Astronaut's emotion is neutral and the current task is blocked.
Therefore, the Robot wants to immediately come up with an alternative
solution.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formation})} As
we discussed earlier, Robot forms new motives according to the result of:

\begin{enumerate}[a)]
  \item appraisal with respect to the shared goal,
  \item reverse appraisal of the Astronaut's emotion,
  \item and the user model of the Astronaut. 
\end{enumerate} 

Although the process of comparing and sorting available motives is similar to
previous example, all of the new motives are different than the motives in that
example. The reason is that each of the above three sources of motives forms a
different motive because of holding different value which is caused by the
ignorance of the Astronaut's actual emotion. For instance, the motive generated
with the influence of appraisal in emotional-awareness example urges the Robot
to postpone asking questions about the alternative solutions while the motive
with the same cause (i.e., appraisal) in emotional-ignorance example urges the
Robot to immediately try to fix the problem and come up with alternative
solutions by asking questions. The Robot, similar to previous example, selects
the most related motive and forms a new intention with respect to the current
status of collaboration. After this whole process, the Robot uses the coping
mechanism to take an action based on the available intention.\\

\noindent\textbf{(Coping)} Based on the current mental state, Robot decides to
use problem-focused coping strategy of seeking information to be able to choose
between two available actions and reduce the current amount of uncertainty.
Therefore, the Robot, without acknowledging the Astronaut's emotion, asks the
Astronaut to choose between two alternative solutions. (B2)\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{B2. Robot:}} I
can help you with the measurement tool, or we can terminate this task.
What do you want me to do?\end{varwidth}}\\

As we mentioned earlier in Section \ref{sec:exp2}, Robot's response does not
make any progress in collaboration status. Hence, Astronaut repeats herself
about the task status (B3).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{B3. Astronaut:}}
As I said the measurement tool does not work properly. We can not continue!
\end{varwidth}}\\

Robot perceives Astronaut's new utterance (B3) while, again, ignores her
frustration. Robot goes through the same process as we mentioned above, and
since Astronaut has just repeated herself, her new utterances do not change the
Robot's mental state. Having the same mental state causes the Robot to ask
similar question (B4).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{B4. Robot:}}
Okay. Do you want me to fix this problem or terminate the task?\end{varwidth}}\\

This time, Robot's question makes an ambiguous assumption for the Astronaut on
whether the Robot can fix the disfunctional measurement tool for her. The
ambiguity of Robot's question does not help Astronaut's frustration and causes
her to ask a clarification question (B5).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B5. Astronaut:}}
Can you fix my measurement tool?\end{varwidth}}\\

Again, the same processes will be run again (similar to what we had above) for
the new utterance of the Astronaut (B5) before the following process. The new
beliefs as the output of the awareness function based on the Astronaut's new
utterance are as it follows. Notice that the Robot still believes that the
Astronaut's emotion is neutral.

\begin{itemize}
  \item[$\bullet$] the precondition associated to the Astronaut's current
  \textit{task} is still \textit{unsatisfied},
  \item[$\bullet$] the status of the Astronaut's current \textit{task} is still
  \textit{blocked},
  \item[$\bullet$] and similarly the status of the \textit{shared goal} is
  still \textit{blocked},
  \item[$\bullet$] however, the Astronaut's question changes the Robot's
  \textit{focus of attention} to fixing the measurement tool,
  \item[$\bullet$] also, the Astronaut's \textit{emotion} is still
  \textit{neutral}.
  \item[$\bullet$] but, her user model \textit{has changed} to having
  medium-autonomy and being highly communicative.
\end{itemize}

%\begin{spacing}{1.15}
%\fontsize{9pt}{10pt}\selectfont
\noindent\textbf{(Collaboration)} The change in the focus of attention to fixing
the measurement tool causes the Robot to check the availability of a recipe to
fix or replace the disfunctioning measurement tool. Similar to previous
example, Robot finds a recipe to replace the measurement tool.\\

\noindent\textbf{(Appraisal)} Robot appraises the possibility of
replacing the measurement tool with respect to: a) the status of the shared
goal, and b) the Astronaut's user model. Robot finds the replacement of the
measurement tool \textit{relevant}, \textit{desirable}, and
\textit{controllable} just as before.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms new motives based on the next task according to the shared plan and
outcome of the appraisal of the possibility of replacing the measurement tool.
Robot forms the corresponding intentions with respect to the new motives.\\

Once again, the Robot decides to take an action based on the recent intention.\\

\noindent\textbf{(Coping)} Based on the current mental state, first, Robot
responds to the Astronaut's question, and then, chooses to negotiate and offer
an alternative action to the Astronaut(B6).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B6. Robot:}} I
cannot fix your measurement tool, but I can fetch another one for you if you
want?\end{varwidth}}\\

Astronaut's strong emotion, shortage of time, and Robot's mismatching answer to
Astronaut's assumption causes the Astronaut to reject the Robot's proposal
(B7).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B7. Astronaut:}}
No, I don't want another measurement tool! We don't have time for
that!\end{varwidth}}\\

After perceiving Astronaut's answer Robot tries to negotiate (using the same
procedure as we discussed above) with the Astronaut to protect the collaboration
and the shared goal from failure. Therefore, the Robot asks about the
possibility of pusuing another task.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B8. Robot:}}
Okay. You want me to terminate this task. Terminating this task can influence
the quality of installing this solar panel which can cause the mission to fail.
Or, do you want us to work on another task? This can help us to install the
panel using your welding tool, but I do not know whether the quality of our
installation will be acceptable.\end{varwidth}}\\

Astronaut terminate the collaboration due to the lack of time and failure in
Robot's collaborative behavior (B9).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{B9. Astronaut:}}
I told you we have this problem and we should terminate the mission! We cannot
continue without the measurement tool!\end{varwidth}}\\

As it was shown in this example, ignoring Astronaut's emotion, first, impacts
the Robot's perception and corrresponding beliefs. The output of collaboration
mechanism remains unchanged in compared to the emotional-awareness example which
is a crucial point in our first two examples. Although the collaboration
mechanism provides the required structural details of collaboration between the
Robot and the Astronaut, these structural details are not enough for saving a
collaboration from a failure. As we continue, we can see ignoring the actual
emotion of the Astronaut causes misfunctioning of the processes in the Theory of
Mind mechanism, i.e., reverse appraisal and user modeling. Comoparing the result
of these two processes with the results in emotional-awareness example, shows
the importance of correctly perceiving a collaborator's emotion. This problem
continues even with appraisal mechanism which maps the Robot's interpretation of
the environment to a wrong emotion. Consequently, all sources of the motivation
mechanism provide incorrect values which drastically influence the formation of
the underlying motives of the required intentions. Finally, the coping mechanism
operates based on wrong newly formed intentions which leads to a totally
different behavior of the Robot in compare to the same turn in
emotional-awareness example. The divergance of the Robot's collaborative
behavior from its successful path contniues among the Robot and the Astronaut's
interaction which increases the required time for achieving the shared goal, and
pepetuates the negative feeling of the Astronaut. The Robot also misses the
right time to begin a negotiation process to save the collaboration from
failure. Therefore, it causes the Astronaut to reject Robot's proposal which
again aggravates the Astronaut's negative emotion. Consequently, the same
collaboration fails even though that the Robot uses the same computational
mechanisms, as we showed above. In Sections \ref{sec:wt-exp3} and
\ref{sec:wt-exp4}, as another example of collaborative behavior, we are going to
show the importance of emotional-awareness and its underlying computational
mechanisms in task delegation procedure.

\subsection{Delegation of a Task (Emotion-Awareness)}
\label{sec:wt-exp3}

This walkthrough example is focusing on delegation of a task by the Astronaut
during collaboration (see also the example in Section \ref{sec:exp3}). In this
example, the explanation between Astronaut's utterance C1 and Robot's utterance
C2 provides the details of how different mechanisms discussed in Section
\ref{sec:computational-framework} are involved making the Robot to show
collaborative behaviors in acceptance of a new delegated task. To avoid
redundant explanations, we refer to similar procedures in previous walkthrough
examples.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C1. Astronaut:}}
I still have some problems with attaching the first panel! We do not have enough
time. You should begin to install the second panel.\end{varwidth}}\\

\noindent\textbf{(Perception)} Robot perceives Astronaut's utterances and
emotion in C1.\\

The perception mechanism forms beliefs based on the Astronaut's utterances and
her emotion (i.e., worriedness) which she has expressed nonverbally. We have
shown some examples of the beliefs formed by perception mechanism in the example
in Section \ref{sec:wt-exp1}.\\

\noindent\textbf{(Collaboration: \textit{Interruption \& Constraint
Management})} Robot infers the interruption and uses the constraint management
process to retrieve required resources and preconditions. Robot also checks
whether there is an available associated recipe for the delegated task.\\

\noindent\textbf{(Theory of Mind: \textit{Reverse Appraisal \& User Modeling})}
Robot uses reverse appraisal to understand the meaning of the Astronaut's
worriedness with respect to the collaborative task status retrieved in previous
step (e.g., precondition status, postcondition status, required resources,
shared goal). Robot also updates the Astronaut's user model and froms belief
that a) Astronaut has \textit{high autonomy}, and b) Astronaut is a highly
communicative collaborator. We have discussed more details about reverse
appraisal and user modeling processes in our example in Section
\ref{sec:wt-exp1}.\\

\noindent\textbf{(Appraisal)} Robot appraises Astronaut's utterances and
emotion. The Robot interprets the Astronaut's utterances and emotional state as
a \textit{relevant}, \textit{unexpected}, \textit{undesirable}, \textit{urgent},
but \textit{controllable} event.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms new motives based on the result of the same processes we discussed in
Section \ref{sec:wt-exp1}, and compares the available motives in the same way
we discussed in that section. Robot forms new intention(s) with respect to the
selected motive.\\

\noindent\textbf{(Coping)} Based on the current mental state, Robot chooses an
emotion-focused coping strategy and decides to acknowledge Astronaut's emotion,
and provide a proper response (C2) without asking questions about the delegated
task.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth}\textit{\textbf{C2. Robot:}}
Okay. Don't worry. I can handle that.\end{varwidth}}\\

Astronaut perceives Robot's acknowledgement of her emotion as well as Robot's
positive response to Astronaut's delegated task. Astronaut knows that the
Robot needs her to help with some of the primitive tasks in her own delegated
task to the Robot. Therefore, the Astronaut, while is still worried about
time, informs the Robot that she will try to quickly finish her current task.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C3. Astronaut:}}
I will try to fix it asap.\end{varwidth}}\\

Robot perceives Astronaut's utterance and emotional expression. The same process
happens from updating beliefs to taking actions as we discussed above or in
previous examples. Since the Robot believes that the Astronaut is still worried
about time, it just informs the Astronaut about some potential questions in the
future. Robot knows about these questions since there are either missing
information according to the partial plan, or required resources and sub-tasks
that can be provided by the Astronaut. Robot chooses a proper utterance about
missing information according to human's emotion (C4).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C4. Robot:}} I
might need to ask some questions while I am installing the second
panel.\end{varwidth}}\\

Astronaut finds the Robot's response appropraite for the delegated task.
Therefore, Robot's proper response mitigates the Astronaut's negative emotion
which was caused by the lack of time for successive installing procedure of the
first and second panels. As a result Astronaut properly responds to the Robot's
needs (C5).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{C5. Astronaut:}}
That's fine. Just let me know.\end{varwidth}}

\subsection{Delegation of a Task (Emotion-Ignorance)}
\label{sec:wt-exp4}

This walkthrough example begins with the same exact utterance as the previous
one in Section \ref{sec:wt-exp3}. This section briefly provides the
corresponding details of the emotion-ignorant example in Section \ref{sec:exp4}
which is focusing on delegation of a task by the Astronaut during collaboration.
In this example, the explanation between Astronaut's utterance D1 and Robot's
utterance D2 provides some details to show that even though the same mechanisms
(discussed in Section \ref{sec:computational-framework}) are used to make the
Robot obtaining collaborative behaviors, ignoring the Astronaut's expressed
emotion changes the output of different computational mechanisms (see also
Section \ref{sec:wt-exp2}) which ultimately causes unsuccessful termination of
the task delegation process. To avoid redundant explanations, we group some of
the Astronaut and Robot's utterances which constitute the representation of
dawdling interaction between them. We also refer to similar procedures in
previous walkthrough examples.\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D1. Astronaut:}}
I still have some problems with attaching the first panel! We do not have enough
time. You should begin to install the second panel.\end{varwidth}}\\

\noindent\textbf{(Perception)} Robot only perceives Astronaut's utterances
(D1). These beliefs are about unsatisfied postcondition of the first task, i.e.,
installing the first solar panel, and Atronaut's proposal of installing the
second panel. We have shown some examples of the beliefs formed by perception
mechanism in Section \ref{sec:wt-exp1}. Also, as we have shown in Section
\ref{sec:wt-exp2} Robot does not perceive Astronaut's emotion (i.e.,
worriedness). Therefore, Robot misses beliefs about Astronaut's emotion.

\noindent\textbf{(Collaboration: \textit{Interruption \& Constraint
Management})} Robot infers the interruption and uses the constraint management
process to retrieve required resources, preconditions. Similar to previous
example, Robot also checks whether there is an available associated recipe for
the delegated task. Ignoring Astronaut's expressed emotion does not change
beliefs formed based on the output of the collaboration mechanism.

\noindent\textbf{(Theory of Mind: \textit{Reverse Appraisal \& User Modeling})}
Similar to the example in Section \ref{sec:wt-exp2}, Robot uses reverse
appraisal to understand the meaning of the Astronaut's emotion with respect to
the collaborative task status. However, since the Robot ignores the actual
Astronaut's emotion, the output of the reverse appraisal does not help the
Robot's inference about its own collaborative behavior (see Section
\ref{sec:wt-exp2}). Similarly, Robot updates the Astronaut's user model based on
wrong beliefs achieved by ignoring Astronaut's expressed emotion. We have
discussed more details about reverse appraisal and user modeling processes in
our example in Section \ref{sec:wt-exp1}, and we have shown similar effects
in Section \ref{sec:wt-exp2}.

\noindent\textbf{(Appraisal)} Robot appraises Astronaut's utterances with a
wrong assumption of her expressing neutral emotion. Consequently, similar to
the example in Section \ref{sec:wt-exp3}, the Robot interprets the Astronaut's
utterances as a \textit{relevant}, \textit{unexpected}, \textit{undesirable},
\textit{urgent}, and \textit{controllable} event.\\

\noindent\textbf{(Motivation: \textit{Motive \& Intention Formations})} Robot
forms and comopares new motives in the same way as we have discussed in previous
examples. However, both processes of the motive formation and comparision are
performed based on false outputs of the other processes (see also Section
\ref{sec:wt-exp2}). Therefore, Robot forms new intention with respect to the
selected motive which can be formed or selected for a wrong reason.

\noindent\textbf{(Coping)} Based on the current mental state (including a
wrong intention), Robot decides to use problem-focused coping strategy of
seeking social support (seeking information) to be able to drop the intention of
working with the Astronaut on the same task. Respectively, Robot asks a quesiton
(D2).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D2. Robot:}}
Okay. I can handle that. Is it okay if I leave the current task though?
\end{varwidth}}\\

The rest of the Robot's utterances from D4 to D10 shows that the Robot wants to
do planning in the most efficient manner by asking so many questions about
missing information according to the shared plan. On the other hand, the
Astronaut who was worried about the lack of time since the beginning tries to
succinctly respond the Robot and successfully delegate the task. However, Robot
by asking several questions instead of understanding Astronaut's mental state
and accepting the delegated task, causes the Astronaut to quickly become
frustrated and express her frustration (D9).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D3. Astronaut:}}
Yes, that's fine. I can finish it by my own.\\

\noindent \textit{\textbf{D4. Robot:}} Okay. But according to the plan, I need
your help when we need to weld the panel. Will you be able to help me?\\

\noindent \textit{\textbf{D5. Astronaut:}} Yes, I will.\\
  
\noindent \textit{\textbf{D6. Robot:}} We also need your welding tool and
supplies as well as the measuring tool to install the second panel.\\
  
\noindent \textit{\textbf{D7. Astronaut:}} Just let me know whenever you need
something! \\
  
\noindent \textit{\textbf{D8. Robot:}} Thanks. I will. How about the conflict
that might occur while I am trying to fix the second panel? You are going to
need my help at the same time.\\
  
\noindent \textit{\textbf{D9. Astronaut:}} Robot, I really don't understand what
you are talking about!\\

\noindent \textit{\textbf{D10. Robot:}} Do you want me to provide some examples?
\end{varwidth}}\\

Robot perceives Astronaut's last utterance and once again ignores her emotion.
This time Robot's tries to provide some examples to clarify its own point
which is even another divergance from achieving the shared goal or planning
for the new delegated task (D10). Finally, Astronaut terminates the
collaboration due to the lack of time and Robot's failure in incoporating
proper collaborative behavior (D11).\\

\noindent\fbox{\begin{varwidth}{0.98\textwidth} \textit{\textbf{D11.
Astronaut:}} We don't have time for this anymore! \end{varwidth}}

\section{Related Work}
\label{sec:related-work}

The prominent collaboration theories are mostly based on plans and joint
intentions
\cite{cohen:teamwork,grosz:plans-discourse,Litman:discourse-commonsense}, and
they were derived from the BDI paradigm developed by Bratman
\cite{bratman:intentions-plans} which is fundamentally reliant on folk
psychology \cite{ravenscroft:folk}. The two theories, Joint Intentions
\cite{cohen:teamwork} and SharedPlans \cite{grosz:plans-discourse}, have been
extensively used to examine and describe teamwork and collaboration. There are
many research focusing on different aspects of collaboration based on different
collaboration theories, i.e., SharedPlans
\cite{grosz:planning-acting,grosz:collaboration,grosz:plans-discourse}, Joint
Intentions \cite{cohen:teamwork}, and hybrid theories of collaboration, e.g.,
STEAM \cite{tambe:flexible-teamwork}. All of the works presented in this section
lack a systematic integration of collaboration theories with some theories
capable of describing underlying collaboration processes. Therefore, they
either do not explain the structure and the underlying processes of
collaboration at the same time, or their appraoch in either or both
of these views is application oriented. For the rest of this section, we provide
a review of several applications based on different prominent collaboration
theories which puts emphasis on the importance of the collaborative robots and
their applicaitons. And at the end, we also provide some related works on
applications of artificial emotions and appraisal theory to express the
importance of their applicability in robots and autonomous agents.

There are some works focusing on the concepts of robot assistants
\cite{clancey:agent-assistants-collaboration}, or teamwork and its challenges in
cognitive and behavioral levels
\cite{nikolaidis:collaboration-joint-action,scerri:prototype-distributed-teams}.
Some researchers have an overall look at a collaboration concept at the
architectural level. In \cite{garcia:collaboration-emotional-awareness} authors
present a collaborative architecture, COCHI, and argue the need to support
emotional-awareness in the design and implementation of groupwares. In
\cite{esau:integrating-emotion-collaboration} authors present the integration of
emotional competence into a cognitive architecture which runs on a robot, MEXI.
In \cite{sofge:collaboration-humanoid-space} authors discuss the challenges of
integrating natural language, gesture understanding and spatial reasoning of a
collaborative humanoid robot situated in space. The importance of communication
during collaboration has also been considered by some researchers from
human-computer interaction and human-robot collaboration
\cite{clair:action-intention-collaboraiton,matignon:verbal-nonverbal-collaboration,rich:discourse}
to theories describing collaborative negotiation, and discourse planning and
structures
\cite{andriessen:disourse-planning,grosz:discourse-structure,sidner:discourse-collaborative-negotiation}.
There are other concepts such as joint actions and commitments
\cite{grosz:intention-dynamics-collaboration}, dynamics of intentions during
collaboration \cite{levesque:acting-together}, and task-based planning providing
more depth in the context of collaboration
\cite{burghart:cognitive-architecture-robot,rich:cea}. The concept of
collaboration has also received attention in the industry and in research in
robotic laboratories \cite{green:collaboration-literature-review}. Some of these
works consider applicability of emotions in their architecture, and some of them
pay more attention to the collaborative aspect of their robot. In the
followings, we review some of the applications of each prominent collaboration
theory.

\textbf{Applications of SharedPlans Theory} -- COLLAGEN
\cite{rich:collaboration-manager,rich:discourse} is the first implemented
system based on the SharedPlans theory. It incorporates certain algorithms for
discourse generation and interpretation, and is able to maintain a segmented
interaction history, which facilitates the discourse between the human user and
the intelligent agent. The model includes two main parts: (1) a representation of
a discourse state and (2) a discourse interpretation algorithm for the
utterances of the user and agent \cite{rickel:discourse-theory-dialogue}. In
\cite{heeman:model-collaboration-referring} Heeman presents a computational
model of how a conversational participant collaborates in order to make a
referring action successful. The model is based on the view of language as
goal-directed behaviour, and in his work, he refers to SharedPlans as part of
the planning and conversation literature. In \cite{lochbaum:plan-models},
Lochbaum and Sidner modify and expand the SharedPlan model of collaborative
behavior \cite{grosz:plans-discourse}. They present an algorithm for updating an
agent’s beliefs about a partial shared plan and describe an initial
implementation of this algorithm in the domain of network management. Lochbaum,
also in \cite{lochbaum:collaborative-planning}, provides a computational model
(based on the collaborative planning framework of SharedPlans
\cite{grosz:collaboration}) for recognizing intentional structure and utilizing
it in discourse processing. In short, she presents a SharedPlans model for
recognizing Discourse Segment Purposes (DSPs) \cite{grosz:plans-discourse}
\cite{sidner:discourse-collaborative-negotiation} and their interrelationships.
CAST (Collaborative Agents for Simulating Teamwork) \cite{yen:cast}
\cite{yin:knowledge-based-sharedplans} is a teamwork framework based on the
SharedPlans theory. CAST focuses on flexibility in dynamic environments and on
proactive information exchange enabled by anticipating what information team
members will need. Petri Nets are used to represent both the team structure and
the teamwork process, i.e., the plans to be executed. Researchers in
\cite{hobbs:microsociology-relationship} discuss developing an ontology of
microsocial concepts for use in an instructional system for teaching
cross-cultural communication. They believe being acquainted with one another is
not a strong enough relationship from which to create a society. Hence, there is
a need for commitment and shared plans (as the basis of social life) to achieve
a shared goal. In this work, Grosz and Sidner's SharedPlans theory
\cite{grosz:plans-discourse} is used to explain the concept of shared plans
within the interpersonal relationships of societies in an industrial
environment. In \cite{hunsberger:auction-collaborative} Hunsberger and Grosz
discuss the idea of whether the rational, utility-maximizing agents should
determine commitment to a group activity when there is an opportunity to
collaborate. They call this problem the ``initial-commitment decision problem''
(ICDP) and provide a mechanism that agents can use to solve the ICDP. They use
the representation of action, act-types and recipes in the SharedPlans theory.
In \cite{zamfirescu:gdss} an integrated agent-based model for Group Decision
Support Systems is proposed and discussed. The decisional model that authors
outline in this paper is based on the SharedPlans theory. Rauenbusch and Grosz
in \cite{rauenbusch:decision-making-planning} formally define a search problem
with search operators that correspond to the team planning decisions.
They provide an algorithm for making the three types of interrelated decisions
by recasting the problem as a search problem. Their model respects the
constraints on mental states specified by the SharedPlans theory of
collaboration. Babaian et. al. in \cite{babaian:writers-assistant} describe
Writer's Aid, a system that deploys AI planning techniques to enable it to serve
as an author's collaborative assistant. While an author writes a document,
Writer's Aid helps in identifying and inserting citation keys and by
autonomously finding and caching potentially relevant papers and their
associated bibliographic information from various on-line sources. They believe
the underlying concepts of SharedPlans is relevent since in collaborative
interfaces like Writer’s Aid, the users establish shared goals with the system
and user and the system both take initiative in satisfying them. In
\cite{montreuil:planning-robot-activity} researchers address high-level robot
planning issues for an interactive cognitive robot that acts in the presence of
or in collaboration with a human partner. They describe a Human Aware Task
Planner (HATP) which is designed to provide socially acceptable plans to achieve
collaborative tasks. They use notions of plans based on SharedPlans theory. In
\cite{sidner:enagagement-robot} Sidner and Dzikovska argue that robots, in order
to participate in conversations with humans, need to make use of conventions of
conversation and the means to be connected to their human counterparts. They
provide an initial research on engagement in human-human interaction and
applications to stationary robots in hosting activities. They believe hosting
activities are collaborative because neither party completely determines the
goals to be undertaken nor the means of reaching the goal. To build a robot
host, they rely on an agent built using COLLAGEN which is implemented based on
the SharedPlans theory.\\

\textbf{Applications of Joint Intentions Theory} -- In \cite{kinny:planned-team}
authors introduce a language for representing joint plans for teams of agents.
They describe how agents can organize the formation of a suitably skilled team
to achieve a joint goal, and they explain how such a team can execute these
plans to generate complex, synchronized team activity. In this paper, authors
adopt the underlying concepts of the Joint Intentions theory as the structure of
their collaborative agents. Breazeal et. al. in \cite{breazeal:humanoid-robots}
present an overview of their work towards building socially intelligent,
cooperative humanoid robots, Leonardo, that can collaborate and learn in
partnership with humans. They employ the Joint Intentions theory of
collaboration to implement the collaborative behaviors while performing a task
in collaboration with humans. In \cite{subramanian:joint-intention-dialogue}
the researchers' goal is to develop an architecture (based on the concepts of
Joint Intentions theory) that can guide an agent during collaborative teamwork.
They describe how a joint intention interpreter that is integrated with a
reasoner over beliefs and communicative acts can form the core of a dialogue
engine. Ultimately, the system engages in dialogue through the planning and
execution of communicative acts necessary to attain the collaborative task at
hand. Mutlu et. al. in \cite{mutlu:coordination-robot} discuss key mechanisms
for effective coordination toward informing the design of communication and
coordination mechanisms for robots. They present two illustrative studies that
explore how robot behavior might be designed to employ these mechanisms
(particularly joint attention and action observation) to improve measure of task
performance in human-robot collaboration. Their work uses Joint Intentions
theory to develop shared task representations and strategies for task
decomposition. The system GRATE* by Jennings
\cite{jennings:joint-intention-hybrid} is based on the Joint Intention theory.
GRATE* provides a rule-based modelling approach to cooperation using the notion
of Joint Responsibilities, which in turn is based on Join Intentions. GRATE* is
geared towards industrial settings in which both agents and the communication
between them can be considered to be reliable.\\

\textbf{Applications of Hybrid Theories} -- The domain independent teamwork
model, STEAM, has been successfully applied to a variety of domains.  From
combat air missions \cite{hill:synthetic-battlefield-aircraft} to robot soccer
\cite{kitano:robocup} to teams supporting human organizations
\cite{pynadath:teamwork-heterogeneous-agents} to rescue response
\cite{scerri:robot-agent-person}. In \cite{marsella:robocup} authors provide
their RoboCup (robotics soccer testbed) in which their focus is on teamwork and
learning challenges. Their research investigation in RobotCup is based on ISI
Synthetic, a team of synthetic soccer-players. They also investigate the use of
STEAM as their model of teamwork. In \cite{kabil:coordination-mechanisms}
researchers propose a behavioral architecture C$^2$BDI that allows the
enhancement of the knowledge sharing using natural language communication
between team members. They define collaborative conversation protocols that
provide proactive behavior to agents for the coordination between team members.
Their agent architecture provides deliberative and conversational behaviors for
collaboration, and it is based on both of the SharedPlans and Joint Intentions
theories.

The applications of different prominent collaboration theories shows the
importance and the applicability of these theories in robots and collaborative
systems. The followings breifly review some of the applications of artificial
emotions and appraisal theory of emotions in robots and autonomous agents.

\textbf{Applications of Artificial Emotions} There are many research areas,
including robotics and autonomous agents, that employ the structure and/or
functions of emotions in their work with a variety of motivations behind
modeling emotions \cite{wehrle:motivations-modeling-emotion}. Some of these
works are inspired by specific psychological theories (we provide several
examples in this section), some are freely using the concept of emotion without
using the theoretical background in social sciences, and some are using a
combination of concepts from the psychological theories. For instance, in PECS
\cite{urban:pecs} which is designed for modeling human behaviors, the agent's
architecture is not based on a certain kind of social or psychological emotion
theory. In fact, it is intentionally designed and described in a way which
enables the integration of a variety of theories. The PECS' design enables an
integrative modeling of physical, emotional, cognitive and social influences
within a component-oriented agent architecture. Also, in
\cite{miranda:teamwork-multiagent-system} the computational architecture which
is designed to provide information about the possible overall behavior of a work
team is not based on any specific theory. Some researchers apply combinations of
emotion theories in their work \cite{kiryazov:modeling-appraisal-pad}. For
instance, in \cite{canamero:designing-activity-selection} Ca$\tilde{n}$amero
shows how an agent can use emotions for activity selection while taking into
account both dimensional and discrete approaches in an action selection
mechanism. We can also see the application of emotion theories in designing
companion robots, robots capable of expressing emotions and social behaviors, as
well as robots which can convey certain types of emotion products, e.g., empathy
\cite{breazeal:expressive-behavior} \cite{leite:empathy-hri}
\cite{paiva:emotion-modeling} \cite{shayganfar:methodology}. Robots also use
emotions theories for automatic affect recognition using different modalities
\cite{hegel:empathic-robot} \cite{zeng:affect-recognition}. Moreover, in some
works, researchers have explored the user's affective state as a mechanism to
adapt the robot's behaviors during the interaction
\cite{breazeal:sociable-robot} \cite{liu:affect-robot-behavior}.

\textbf{Applications of Appraisal Theory} -- The emphasis of models derived from
appraisal theories of emotion is on making appraisal the central process.
Computational appraisal models have been applied to a variety of uses including
contributions to psychology, robotics, AI, and HCI. For instance, Marsella and
Gratch have used EMA \cite{marsella:ema-process-model} to generate specific
predictions about how human subjects will appraise and cope with emotional
situations and argue that empirical tests of these predictions have implications
for psychological appraisal theory \cite{gratch:assessing-appraisal}
\cite{marsella:assessing-coping}. There are several examples in artificial
intelligence and robotics of applying appraisal theory
\cite{adam:bdi-emotional-companion} \cite{kim:model-hri-appraisal}
\cite{marsella:ema-process-model}. In robotics, appraisal theory has been used
to establish and maintain a better interaction between a robot and a human. For
instance in \cite{kim:model-hri-appraisal} researchers provide their
computational model of emotion generation based on appraisal theory to have a
positive human-robot interaction experience. In
\cite{sander:systems-approach-appraisal} authors describe a system approach to
appraisal processes based on Scherer's work on appraisal and the Component
Process Model \cite{scherer:nature-function-emotion}. They show how the temporal
unfolding of emotions can be experimentally tested. They also lay out a general
domain-independent computational model of appraisal and coping. In
\cite{vogiatzis:robot-museum} researchers consider their robot's (INDIGO)
emotion, speech and facial expressions as a key point to establish an effective
communication between the robot and a human during their interaction. They apply
concepts of appraisal theory in INDIGO's emotion modeling. MAGGIE, a sociable
robot, also applies the appraisal theory of emotions to consider fear in its
decision making system \cite{castro:autonomous-robot-fear}. Velasquez developed
Cathexis which is a distributed computational model for generation of emotions
and their influence in the behavior of the autonomous agents
\cite{velasquez:emotions-motivations-agents}. The emotion model in this work is
based on Roseman's work on appraisal theory. Marinier and Laird in
\cite{marinier:emotion-reinforcement} focus on the functional benefits of
emotion in a cognitive system. In this work, they integrate their emotion theory
(which is based on the appraisal theory) with Soar cognitive architecture, and
use emotional feedback to drive reinforcement learning. In
\cite{hudlicka:emotinos-reasons} Hudlicka provides a model of a generic
mechanism mediating the affective influences on cognition based on cognitive
appraisal. This model is implemented within a domain-independent
cognitive-affective architecture (MAMAID). In the virtual agents community,
empathy is a research topic that has received much attention in the last decade
\cite{brave:emotion-hci} \cite{scott:modeling-empathy-agent}
\cite{paiva:agent-care} \cite{prendinger:empathic-companion}
\cite{bickmore:longterm-relationship}. In \cite{pontier:women-robot-men}
researchers developed an agent with capability of affective decision-making
based on appraisal theory to establish an affective relationship with its users.
Then, they compared the performance of their agent with a human (based on a WoZ
study) in a speed-dating experiment.

\section{Conclusion and Future Work}

In this paper, first, we argued the missing part of computational collaboration
theories and the need for a theory explaining the undelying processes involved
in a collaboration. Then, we discussed the importance of social functions of
emotions and how emotions are involved in social contexts to reveal or enrich
the meaning of interactants' messages through communication. In fact, there is a
correspondence between what a collaboration needs and what social functions of
emotions provide within a social context. Next, in Section
\ref{sec:example-scenario}, we provided four hypothetical examples in two pairs.
Each pair of examples was about a distinct collaborative behavior. The first
pair was about agreeing on a shared goal between a robot and an astronaut, and
the second pair was about delegation of a new task to the robot by the
astronaut. Each pair of these examples contains a successful collaboration
because of the robot being aware of the astronaut's emotion, and a failure in
collaboration as the consequence of the robot ignoring the astraonuat's
emotions. We provided a brief description for each example as well as the
utterances of both the robot and the astronaut during their collaboration. These
examples could illustrate the importance of emotional-awareness to attain a
successful collaborative behavior. Then, in Section
\ref{sec:computational-framework}, we continued by briefly introduing main
components of \textit{Affective Motivational Collaboration Thoery} as our
computational framework which incorporates emotion-regulated mechanisms. This
framework let us to describe the same examples in more details (in Section
\ref{sec:wtce}) in a way that we could explain what mechanisms or their
underlying processes, and how they are involved to help the robot obtaining a
collaborative behavior by observing the astronaut's emotions. At last, in
Section \ref{sec:related-work}, we provided some related works in which they are
using comoputaitonal collaboration and emotion theories.

We have also implemented the rules associated to these examples using JESS (Java
Expert System Shell) which is a rule engine for the Java platform. In our
current implementation we have categorized the rules in different modules
associated with the mechanisms and the underlying processes in \textit{Affective
Motivational Collaboration Thoery} (see Fig. \ref{fig:theory}). In our future
work, we will implement our algorithms for each individual mechanism to be able
to automatically generate the reauired facts within each mechanism to fire the
existing rules. Ultimately, we are going to offer a platform which operates
based on collaboration structure discussed by ShredPlans theory
\cite{grosz:discourse-structure}, and employs some emotion-driven processes such
as appriasal process in \cite{marsella:ema-process-model} to enable a robot to
obtain and demonstrate collaborative behaviors.

%\label{sec:2} ~\ref{sec:1}

%\paragraph{Paragraph headings} 


% For one-column wide figures use
%\begin{figure}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
%  \includegraphics{example.eps}
% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:1}       % Give a unique label
%\end{figure}
%
% For two-column wide figures use
%\begin{figure*}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
%  \includegraphics[width=0.75\textwidth]{example.eps}
% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:2}       % Give a unique label
%\end{figure*}
%



% For tables use
%\begin{table}
% table caption is above the table
%\caption{Please write your table caption here}
%\label{tab:1}       % Give a unique label
% For LaTeX tables use
%\begin{tabular}{lll}
%\hline\noalign{\smallskip}
%first & second & third  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%number & number & number \\
%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}


%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)

% Format for books

%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)

% etc
%\end{thebibliography}

\bibliographystyle{abbrv}
\bibliography{mshayganfar.bib}

\end{document}
% end of file template.tex

